{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58327de",
   "metadata": {},
   "source": [
    "# Probando Dominio Adversarial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2855f9",
   "metadata": {},
   "source": [
    "chestmnist + pathmnist -> SSL\n",
    "chestmnist(etiquetado) + breastmnist -> DANN\n",
    "bloodmnist -> inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a964ae8",
   "metadata": {},
   "source": [
    "# SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e436bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a70cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "transform = SimCLRTransform(input_size=28)\n",
    "dataset = LightlyDataset(\n",
    "    input_dir='/lustre/proyectos/p032/datasets/images/tmp',\n",
    "    transform=transform)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ab55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Modelo\n",
    "\n",
    "# --- 2. Backbone ---\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # Quitar la capa final\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class SimCLRProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim): # <-- Recibe 2048\n",
    "        super().__init__()\n",
    "        hidden_dim = input_dim // 4 # Ej: 2048 // 4 = 512\n",
    "        \n",
    "        # ¬°CORRECTO! Usa el 'input_dim' (2048)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class MoCoLightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, \n",
    "                 lr=0.0003, \n",
    "                 temperature=0.1, \n",
    "                 momentum=0.999, \n",
    "                 queue_size=65536,\n",
    "                 input_dim=512, \n",
    "                 output_dim=128):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('lr', 'temperature', 'momentum', 'queue_size', 'input_dim', 'output_dim')\n",
    "\n",
    "        # 1. Crear los encoders de Consulta (q) y Clave (k)\n",
    "        # El encoder_q es el que se entrena con backprop\n",
    "        self.encoder_q = nn.Sequential(\n",
    "            backbone,\n",
    "            nn.Flatten(start_dim=1), # <-- APLANA a (B, 2048)\n",
    "            SimCLRProjectionHead(self.hparams.input_dim, self.hparams.output_dim)\n",
    "        )\n",
    "        \n",
    "        # El encoder_k es el encoder de momentum\n",
    "        self.encoder_k = deepcopy(self.encoder_q)\n",
    "\n",
    "        # Congelar los par√°metros del encoder_k. No se entrenan con el optimizador.\n",
    "        for param in self.encoder_k.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 2. Crear la fila (queue)\n",
    "        # \n",
    "        self.register_buffer(\"queue\", torch.randn(self.hparams.output_dim, self.hparams.queue_size))\n",
    "        self.queue = F.normalize(self.queue, dim=0)\n",
    "        \n",
    "        # Puntero para saber d√≥nde insertar en la fila\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\" Actualizaci√≥n de momentum para el encoder_k \"\"\"\n",
    "        # \n",
    "        m = self.hparams.momentum\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * m + param_q.data * (1. - m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        \"\"\" Saca el batch m√°s antiguo de la fila y a√±ade el nuevo batch de 'keys' \"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        \n",
    "        # Asegurarse de que el batch cabe\n",
    "        assert self.hparams.queue_size % batch_size == 0 \n",
    "\n",
    "        # Reemplazar las claves en la fila\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.hparams.queue_size  # Mover el puntero\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    def forward(self, x):\n",
    "        # El forward ahora solo se usa para inferencia (ej. clasificaci√≥n lineal)\n",
    "        # Devuelve solo las caracter√≠sticas del backbone\n",
    "        return self.encoder_q[0](x).flatten(start_dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (im_q, im_k), _, _ = batch # (x0, x1) ahora son im_q (consulta) e im_k (clave)\n",
    "        \n",
    "        # 1. Computar features de consulta (q)\n",
    "        q = self.encoder_q(im_q)\n",
    "        q = F.normalize(q, dim=1)\n",
    "\n",
    "        # 2. Computar features de clave (k)\n",
    "        with torch.no_grad():\n",
    "            # Actualizar el encoder de clave (momentum)\n",
    "            self._momentum_update_key_encoder()\n",
    "            \n",
    "            # Obtener las claves (sin gradiente)\n",
    "            k = self.encoder_k(im_k)\n",
    "            k = F.normalize(k, dim=1)\n",
    "\n",
    "        # 3. Calcular la p√©rdida\n",
    "        loss = self.moco_loss(q, k)\n",
    "        \n",
    "        # 4. Actualizar la fila\n",
    "        self._dequeue_and_enqueue(k)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def moco_loss(self, q, k):\n",
    "        # q: NxC (consultas)\n",
    "        # k: NxC (claves positivas)\n",
    "        # queue: CxK (claves negativas)\n",
    "\n",
    "        # Logits positivos (N, 1)\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        \n",
    "        # Logits negativos (N, K)\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        # Logits totales (N, 1+K)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        \n",
    "        # Aplicar temperatura\n",
    "        logits /= self.hparams.temperature\n",
    "\n",
    "        # Etiquetas (siempre es la primera columna, la positiva)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # IMPORTANTE: El optimizador SOLO debe entrenar el encoder_q\n",
    "        # Los par√°metros del encoder_k se actualizan por momentum.\n",
    "        \n",
    "        # El paper us√≥ AdamW [cite: 735]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.encoder_q.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=1e-5 # El paper prob√≥ 1e-5 [cite: 736]\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /lustre/proyectos/p032/env/lib64/python3.9/site-pack ...\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     11\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     12\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# detecta GPU autom√°ticamente\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,           \u001b[38;5;66;03m# cambia a 4 si quieres usar todas tus GPUs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# --- 6. Entrenamiento ---\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- 7. Guardar backbone al final ---\u001b[39;00m\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMG_backbone_ssl.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:560\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:110\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches processes that run the given function in parallel.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mThe function is allowed to have a return value. However, when all processes join, only the return value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 110\u001b[0m     \u001b[43m_check_bad_cuda_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     _check_missing_main_guard()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/strategies/launchers/multiprocessing.py:208\u001b[0m, in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE:\n\u001b[1;32m    207\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You will have to restart the Python kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
     ]
    }
   ],
   "source": [
    "# --- 4. Inicializar modelo Lightning ---\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "logger = CSVLogger(save_dir=\"logs\", name=\"mo_co_run\")\n",
    "\n",
    "model = MoCoLightning(\n",
    "    backbone=backbone,\n",
    "    lr=0.0003,          # El LR que ten√≠as\n",
    "    temperature=0.1,    # La temperatura que ten√≠as\n",
    "    queue_size=8192     # Un valor m√°s peque√±o si 65536 da OOM\n",
    ")\n",
    "\n",
    "# --- 5. Entrenador Lightning ---\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"gpu\",  # detecta GPU autom√°ticamente\n",
    "    devices=-1,           # cambia a 4 si quieres usar todas tus GPUs\n",
    "    log_every_n_steps=10,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# --- 6. Entrenamiento ---\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "# --- 7. Guardar backbone al final ---\n",
    "torch.save(model.backbone.state_dict(), \"MG_backbone_ssl.pth\")\n",
    "print(f\"El log de p√©rdidas por √©poca se guard√≥ en: {logger.log_dir}/metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
