{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58327de",
   "metadata": {},
   "source": [
    "# Probando Dominio Adversarial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2855f9",
   "metadata": {},
   "source": [
    "chestmnist + pathmnist -> SSL\n",
    "chestmnist(etiquetado) + breastmnist -> DANN\n",
    "bloodmnist -> inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a964ae8",
   "metadata": {},
   "source": [
    "# SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e436bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a70cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "color_jitter = transforms.ColorJitter(\n",
    "    0.5 * 0.8,  # brillo\n",
    "    0.5 * 0.8,  # contraste\n",
    "    0.5 * 0.8,  # saturaci√≥n\n",
    "    0.2 * 0.8,  # tono\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = LightlyDataset(\n",
    "    input_dir='/lustre/proyectos/p032/datasets/images/tmp',\n",
    "    transform=transform)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ab55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Modelo\n",
    "\n",
    "# --- 2. Backbone ---\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # Quitar la capa final\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class SimCLRProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim): # <-- Recibe 2048\n",
    "        super().__init__()\n",
    "        hidden_dim = input_dim // 4 # Ej: 2048 // 4 = 512\n",
    "        \n",
    "        # ¬°CORRECTO! Usa el 'input_dim' (2048)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class MoCoLightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, \n",
    "                 lr=0.0003, \n",
    "                 temperature=0.1, \n",
    "                 momentum=0.999, \n",
    "                 queue_size=65536,\n",
    "                 input_dim=512, \n",
    "                 output_dim=128):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('lr', 'temperature', 'momentum', 'queue_size', 'input_dim', 'output_dim')\n",
    "\n",
    "        # 1. Crear los encoders de Consulta (q) y Clave (k)\n",
    "        # El encoder_q es el que se entrena con backprop\n",
    "        self.encoder_q = nn.Sequential(\n",
    "            backbone,\n",
    "            nn.Flatten(start_dim=1), # <-- APLANA a (B, 2048)\n",
    "            SimCLRProjectionHead(self.hparams.input_dim, self.hparams.output_dim)\n",
    "        )\n",
    "        \n",
    "        # El encoder_k es el encoder de momentum\n",
    "        self.encoder_k = deepcopy(self.encoder_q)\n",
    "\n",
    "        # Congelar los par√°metros del encoder_k. No se entrenan con el optimizador.\n",
    "        for param in self.encoder_k.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 2. Crear la fila (queue)\n",
    "        # \n",
    "        self.register_buffer(\"queue\", torch.randn(self.hparams.output_dim, self.hparams.queue_size))\n",
    "        self.queue = F.normalize(self.queue, dim=0)\n",
    "        \n",
    "        # Puntero para saber d√≥nde insertar en la fila\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\" Actualizaci√≥n de momentum para el encoder_k \"\"\"\n",
    "        # \n",
    "        m = self.hparams.momentum\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * m + param_q.data * (1. - m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        \"\"\" Saca el batch m√°s antiguo de la fila y a√±ade el nuevo batch de 'keys' \"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        \n",
    "        # Asegurarse de que el batch cabe\n",
    "        assert self.hparams.queue_size % batch_size == 0 \n",
    "\n",
    "        # Reemplazar las claves en la fila\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.hparams.queue_size  # Mover el puntero\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    def forward(self, x):\n",
    "        # El forward ahora solo se usa para inferencia (ej. clasificaci√≥n lineal)\n",
    "        # Devuelve solo las caracter√≠sticas del backbone\n",
    "        return self.encoder_q[0](x).flatten(start_dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (im_q, im_k), _, _ = batch # (x0, x1) ahora son im_q (consulta) e im_k (clave)\n",
    "        \n",
    "        # 1. Computar features de consulta (q)\n",
    "        q = self.encoder_q(im_q)\n",
    "        q = F.normalize(q, dim=1)\n",
    "\n",
    "        # 2. Computar features de clave (k)\n",
    "        with torch.no_grad():\n",
    "            # Actualizar el encoder de clave (momentum)\n",
    "            self._momentum_update_key_encoder()\n",
    "            \n",
    "            # Obtener las claves (sin gradiente)\n",
    "            k = self.encoder_k(im_k)\n",
    "            k = F.normalize(k, dim=1)\n",
    "\n",
    "        # 3. Calcular la p√©rdida\n",
    "        loss = self.moco_loss(q, k)\n",
    "        \n",
    "        # 4. Actualizar la fila\n",
    "        self._dequeue_and_enqueue(k)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def moco_loss(self, q, k):\n",
    "        # q: NxC (consultas)\n",
    "        # k: NxC (claves positivas)\n",
    "        # queue: CxK (claves negativas)\n",
    "\n",
    "        # Logits positivos (N, 1)\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        \n",
    "        # Logits negativos (N, K)\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        # Logits totales (N, 1+K)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        \n",
    "        # Aplicar temperatura\n",
    "        logits /= self.hparams.temperature\n",
    "\n",
    "        # Etiquetas (siempre es la primera columna, la positiva)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # IMPORTANTE: El optimizador SOLO debe entrenar el encoder_q\n",
    "        # Los par√°metros del encoder_k se actualizan por momentum.\n",
    "        \n",
    "        # El paper us√≥ AdamW [cite: 735]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.encoder_q.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=1e-5 # El paper prob√≥ 1e-5 [cite: 736]\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc7c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /lustre/proyectos/p032/env/lib64/python3.9/site-pack ...\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "W1021 07:47:14.261246 316819 torch/multiprocessing/spawn.py:169] Terminating process 316912 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 598, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 967, in _run\n    self.strategy.setup_environment()\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 153, in setup_environment\n    super().setup_environment()\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 129, in setup_environment\n    self.accelerator.setup_device(self.root_device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/accelerators/cuda.py\", line 46, in setup_device\n    _check_cuda_matmul_precision(device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/accelerators/cuda.py\", line 161, in _check_cuda_matmul_precision\n    if not torch.cuda.is_available() or not _is_ampere_or_later(device):\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/accelerators/cuda.py\", line 155, in _is_ampere_or_later\n    major, _ = torch.cuda.get_device_capability(device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 600, in get_device_capability\n    prop = get_device_properties(device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 616, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     15\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     16\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# detecta GPU autom√°ticamente\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# --- 6. Entrenamiento ---\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# --- 7. Guardar backbone al final ---\u001b[39;00m\n\u001b[1;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mencoder_q[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMG_backbone_ssl.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:560\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py:215\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout, grace_period)\u001b[0m\n\u001b[1;32m    213\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_index\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 598, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 967, in _run\n    self.strategy.setup_environment()\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 153, in setup_environment\n    super().setup_environment()\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 129, in setup_environment\n    self.accelerator.setup_device(self.root_device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/accelerators/cuda.py\", line 46, in setup_device\n    _check_cuda_matmul_precision(device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/accelerators/cuda.py\", line 161, in _check_cuda_matmul_precision\n    if not torch.cuda.is_available() or not _is_ampere_or_later(device):\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/accelerators/cuda.py\", line 155, in _is_ampere_or_later\n    major, _ = torch.cuda.get_device_capability(device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 600, in get_device_capability\n    prop = get_device_properties(device)\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 616, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Inicializar modelo Lightning ---\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "logger = CSVLogger(save_dir=\"logs\", name=\"mo_co_run\")\n",
    "\n",
    "model = MoCoLightning(\n",
    "    backbone=backbone,\n",
    "    lr=0.0003,          # El LR que ten√≠as\n",
    "    temperature=0.1,    # La temperatura que ten√≠as\n",
    "    queue_size=8192     # Un valor m√°s peque√±o si 65536 da OOM\n",
    ")\n",
    "\n",
    "# --- 5. Entrenador Lightning ---\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",  # detecta GPU autom√°ticamente\n",
    "    devices=2,           # cambia a 4 si quieres usar todas tus GPUs\n",
    "    log_every_n_steps=10,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# --- 6. Entrenamiento ---\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "# --- 7. Guardar backbone al final ---\n",
    "torch.save(model.encoder_q[0].state_dict(), \"MG_backbone_ssl.pth\")\n",
    "print(f\"El log de p√©rdidas por √©poca se guard√≥ en: {logger.log_dir}/metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
