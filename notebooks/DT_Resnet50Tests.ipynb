{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0d96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "# Import Dataset to inherit from it\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import for the demonstration code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "# --- End PyTorch Imports ---\n",
    "\n",
    "class ImageDatasetWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset compatible con PyTorch que escanea subdirectorios de clases.\n",
    "    Hereda de torch.utils.data.Dataset.\n",
    "    Devuelve etiquetas como vectores one-hot (np.ndarray).\n",
    "    \n",
    "    隆NUEVO! Tambi茅n crea una lista 'self.targets' con etiquetas enteras\n",
    "    (ej. 0, 1, 2) para ser usada por 'sklearn.model_selection.train_test_split'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, transform: Any = None):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset, escanea el directorio y crea el mapa de 铆ndices.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # data_index almacenar谩 (filepath, one_hot_label)\n",
    "        self.data_index: List[Tuple[str, np.ndarray]] = []\n",
    "        \n",
    "        # --- 隆CORRECCIN AADIDA AQU! ---\n",
    "        # self.targets almacenar谩 el 铆ndice entero (0, 1, 2...) para la estratificaci贸n\n",
    "        self.targets: List[int] = []\n",
    "        # --- FIN DE LA CORRECCIN ---\n",
    "        \n",
    "        self.class_names: List[str] = []\n",
    "        self.class_to_label: Dict[str, np.ndarray] = {}\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"\n",
    "        Escanea el directorio ra铆z en busca de carpetas de clases y rellena \n",
    "        data_index (para los datos) y targets (para la divisi贸n).\n",
    "        \"\"\"\n",
    "        print(f\"Escaneando directorio: {self.root_dir}\")\n",
    "\n",
    "        # 1. Descubrir nombres de clases (subdirectorios)\n",
    "        subdirs = [d for d in os.listdir(self.root_dir)\n",
    "                   if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        self.class_names = sorted(subdirs)\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        if num_classes == 0:\n",
    "            raise ValueError(f\"No se encontraron subdirectorios de clases en {self.root_dir}\")\n",
    "\n",
    "        # 2. Crear mapeo class_to_label (para arrays one-hot)\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            one_hot = np.zeros(num_classes, dtype=np.float32)\n",
    "            one_hot[i] = 1.0\n",
    "            self.class_to_label[class_name] = one_hot\n",
    "\n",
    "        print(f\"Se encontraron {num_classes} clases: {self.class_names}\")\n",
    "\n",
    "        # 3. Rellenar la lista de 铆ndices maestros\n",
    "        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "        for class_index, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            one_hot_label = self.class_to_label[class_name]\n",
    "\n",
    "            # Listar archivos en el directorio de la clase\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.lower().endswith(image_extensions):\n",
    "                    filepath = os.path.join(class_path, filename)\n",
    "                    # Almacenar (filepath, one_hot_label)\n",
    "                    self.data_index.append((filepath, one_hot_label))\n",
    "                    \n",
    "                    # --- 隆CORRECCIN AADIDA AQU! ---\n",
    "                    # Almacenar el 铆ndice entero (0, 1, 2...)\n",
    "                    self.targets.append(class_index)\n",
    "                    # --- FIN DE LA CORRECCIN ---\n",
    "\n",
    "        print(f\"Total de im谩genes indexadas: {len(self.data_index)}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Devuelve el n煤mero total de items (im谩genes) en el dataset.\"\"\"\n",
    "        return len(self.data_index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Recupera la imagen y su etiqueta one-hot correspondiente.\n",
    "        Aplica transformaciones si se proporcionan.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.data_index) or idx < 0:\n",
    "            raise IndexError(\"ndice fuera de rango\")\n",
    "\n",
    "        filepath, label_vector = self.data_index[idx]\n",
    "\n",
    "        # 1. Cargar la imagen con PIL\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar la imagen {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 2. Aplicar transformaciones (ej. ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Devuelve la imagen transformada y el vector one-hot\n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc5e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el dataset completo para indexar...\n",
      "Escaneando directorio: /lustre/proyectos/p032/datasets/images/3kvasir\n",
      "Se encontraron 3 clases: ['normal-cecum', 'normal-pylorus', 'normal-z-line']\n",
      "Total de im谩genes indexadas: 1500\n",
      "Total de im谩genes encontradas: 1500\n",
      "Realizando primera divisi贸n (estratificada)...\n",
      "Realizando segunda divisi贸n (estratificada)...\n",
      "\n",
      "--- 隆Divisi贸n completada! ---\n",
      "Total:      1500\n",
      "Set Train:  1049\n",
      "Set Val:    226\n",
      "Set Test:   225\n",
      "\n",
      "DataLoaders estratificados (train, val, test) creados.\n",
      "\n",
      "Verificando distribuci贸n (ejemplo):\n",
      "  Train: ['33.37%', '33.37%', '33.27%']\n",
      "  Val:   ['33.19%', '33.19%', '33.63%']\n",
      "  Test:  ['33.33%', '33.33%', '33.33%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Un NUEVO Dataset Wrapper (m谩s simple)\n",
    "# ---------------------------------------------------------------\n",
    "class PreSplitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset que acepta una lista de datos (filepath, label) \n",
    "    pre-dividida en su constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list: List[Tuple[str, np.ndarray]], transform: Any = None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Obtener el filepath y la etiqueta de la lista\n",
    "        filepath, label_vector = self.data_list[idx]\n",
    "\n",
    "        # Cargar la imagen\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            raise\n",
    "            \n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label_vector\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Configuraci贸n y Proceso de Divisi贸n\n",
    "# ---------------------------------------------------------------\n",
    "# --- Configuraci贸n ---\n",
    "dataset_root = \"/lustre/proyectos/p032/datasets/images/3kvasir\"\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# Definir los ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15 # (debe sumar 1.0)\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 1. Cargar el dataset COMPLETO ---\n",
    "print(\"Cargando el dataset completo para indexar...\")\n",
    "# (Necesitamos la clase 'ImageDatasetWrapper' original para esto)\n",
    "# (He a帽adido .targets a la clase para que esto funcione)\n",
    "full_dataset = ImageDatasetWrapper(root_dir=dataset_root)\n",
    "\n",
    "# Extraer los datos y las etiquetas para sklearn\n",
    "# data_index es List[Tuple[str, np.ndarray]]\n",
    "# targets es List[int] (ej. 0, 1, 2, 0, 1...)\n",
    "all_data = full_dataset.data_index \n",
    "all_targets = full_dataset.targets \n",
    "\n",
    "if len(all_data) == 0:\n",
    "    raise RuntimeError(\"Error: No se encontraron datos en el dataset.\")\n",
    "\n",
    "print(f\"Total de im谩genes encontradas: {len(all_data)}\")\n",
    "\n",
    "# --- 2. Primera Divisi贸n (Train+Val vs Test) ---\n",
    "# Dividimos el 85% para (train+val) y el 15% para test\n",
    "print(\"Realizando primera divisi贸n (estratificada)...\")\n",
    "train_val_data, test_data, train_val_targets, test_targets = train_test_split(\n",
    "    all_data,\n",
    "    all_targets,\n",
    "    test_size=TEST_RATIO,\n",
    "    stratify=all_targets, # 隆La clave es esta!\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# --- 3. Segunda Divisi贸n (Train vs Val) ---\n",
    "# Dividimos (train+val) en train y val\n",
    "# El ratio debe recalcularse: VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "val_split_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "\n",
    "print(\"Realizando segunda divisi贸n (estratificada)...\")\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(\n",
    "    train_val_data,\n",
    "    train_val_targets,\n",
    "    test_size=val_split_ratio,\n",
    "    stratify=train_val_targets, # Estratificar de nuevo\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n--- 隆Divisi贸n completada! ---\")\n",
    "print(f\"Total:      {len(all_data)}\")\n",
    "print(f\"Set Train:  {len(train_data)}\")\n",
    "print(f\"Set Val:    {len(val_data)}\")\n",
    "print(f\"Set Test:   {len(test_data)}\")\n",
    "\n",
    "# --- 4. Crear los Datasets y DataLoaders ---\n",
    "\n",
    "# Aplicar la transformaci贸n a cada set\n",
    "train_dataset = PreSplitDataset(train_data, transform=transform)\n",
    "val_dataset = PreSplitDataset(val_data, transform=transform)\n",
    "test_dataset = PreSplitDataset(test_data, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"\\nDataLoaders estratificados (train, val, test) creados.\")\n",
    "\n",
    "# --- 5. (Opcional) Verificar la distribuci贸n de clases ---\n",
    "print(\"\\nVerificando distribuci贸n (ejemplo):\")\n",
    "\n",
    "def get_class_counts(targets_list):\n",
    "    counts = np.bincount(targets_list)\n",
    "    return [f\"{count/len(targets_list)*100:.2f}%\" for count in counts]\n",
    "    \n",
    "print(f\"  Train: {get_class_counts(train_targets)}\")\n",
    "print(f\"  Val:   {get_class_counts(val_targets)}\")\n",
    "print(f\"  Test:  {get_class_counts(test_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b9f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "N煤mero de clases: 3\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURACIN INICIAL ---\n",
    "# ==========================================================\n",
    "# PATH_MODELO_SSL = \"/lustre/proyectos/p032/models/multi_pretext_model2.ckpt\" # No se usa\n",
    "# MODEL_PATH = \"/lustre/home/opacheco/MEDA_Challenge/models/221025MG_backbone.ssl.pth\" # No se usa\n",
    "\n",
    "# 驴Cu谩ntas clases tiene tu dataset de PRUEBA?\n",
    "NUM_CLASES = 3 # Esto sigue siendo correcto para tu 3kvasir\n",
    "\n",
    "# Par谩metros (隆Importante usar los mismos!)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_DE_PRUEBA = 10\n",
    "LEARNING_RATE = 0.001 # Este LR se us贸 para el Linear Probe, 隆mantenerlo!\n",
    "# JIGSAW_N = 2 # No aplica aqu铆\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "print(f\"N煤mero de clases: {NUM_CLASES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caae3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando ResNet-50 pre-entrenado en ImageNet...\n",
      "隆Backbone ResNet-50 (ImageNet) cargado!\n"
     ]
    }
   ],
   "source": [
    "# --- Cargar el Backbone Baseline (ResNet-50 ImageNet) ---\n",
    "\n",
    "print(\"Cargando ResNet-50 pre-entrenado en ImageNet...\")\n",
    "try:\n",
    "    # Carga la arquitectura ResNet-50 CON pesos pre-entrenados de ImageNet\n",
    "    resnet_imagenet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2) # V2 es la m谩s reciente\n",
    "except AttributeError:\n",
    "    print(\"...usando fallback 'pretrained=True' por versi贸n de torchvision.\")\n",
    "    resnet_imagenet = models.resnet50(pretrained=True)\n",
    "\n",
    "# Crea el backbone quitando la capa final de 1000 clases\n",
    "baseline_backbone = nn.Sequential(*list(resnet_imagenet.children())[:-1])\n",
    "print(\"隆Backbone ResNet-50 (ImageNet) cargado!\")\n",
    "\n",
    "# Mover a GPU\n",
    "baseline_backbone = baseline_backbone.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98995d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Linear Probing (ResNet-50 Baseline) creado.\n"
     ]
    }
   ],
   "source": [
    "# --- Crear el Modelo para Linear Probing ---\n",
    "\n",
    "# Congelar todo el backbone baseline\n",
    "for param in baseline_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- 隆CAMBIO IMPORTANTE! ---\n",
    "# La salida de ResNet-50 es 2048, no 512\n",
    "in_features = 2048\n",
    "# Crear la cabeza lineal\n",
    "linear_head = nn.Linear(in_features, NUM_CLASES)\n",
    "\n",
    "# Clase para el modelo combinado (Backbone + Cabeza)\n",
    "class LinearProbingModel(nn.Module):\n",
    "    def __init__(self, backbone, linear_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.linear_head = linear_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 隆IMPORTANTE! Asegurar que el backbone est茅 en modo eval\n",
    "        self.backbone.eval()\n",
    "        with torch.no_grad(): # No calcular gradientes para el backbone\n",
    "            feats = self.backbone(x)          # [B, 2048, 1, 1]\n",
    "\n",
    "        feats = feats.view(feats.size(0), -1)  # Flatten -> [B, 2048]\n",
    "        out = self.linear_head(feats)     # [B, NUM_CLASES]\n",
    "        return out\n",
    "\n",
    "# Crear la instancia del modelo final\n",
    "model = LinearProbingModel(baseline_backbone, linear_head).to(DEVICE)\n",
    "\n",
    "# Configurar Loss y Optimizador (solo para la cabeza lineal)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.linear_head.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Modelo Linear Probing (ResNet-50 Baseline) creado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7095946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\n",
      "Epoch 1/10 - Train Loss: 0.7335 - Val Loss: 0.4293 - Val Acc: 95.13%\n",
      "Epoch 2/10 - Train Loss: 0.3290 - Val Loss: 0.2383 - Val Acc: 96.46%\n",
      "Epoch 3/10 - Train Loss: 0.2024 - Val Loss: 0.1690 - Val Acc: 96.90%\n",
      "Epoch 4/10 - Train Loss: 0.1512 - Val Loss: 0.1351 - Val Acc: 97.35%\n",
      "Epoch 5/10 - Train Loss: 0.1232 - Val Loss: 0.1149 - Val Acc: 97.35%\n",
      "Epoch 6/10 - Train Loss: 0.1046 - Val Loss: 0.1011 - Val Acc: 97.79%\n",
      "Epoch 7/10 - Train Loss: 0.0914 - Val Loss: 0.0912 - Val Acc: 98.23%\n",
      "Epoch 8/10 - Train Loss: 0.0814 - Val Loss: 0.0835 - Val Acc: 98.23%\n",
      "Epoch 9/10 - Train Loss: 0.0736 - Val Loss: 0.0771 - Val Acc: 98.23%\n",
      "Epoch 10/10 - Train Loss: 0.0676 - Val Loss: 0.0718 - Val Acc: 98.23%\n",
      "Entrenamiento de la cabeza finalizado.\n"
     ]
    }
   ],
   "source": [
    "# --- ENTRENAR LA CABEZA LINEAL CON VALIDACIN ---\n",
    "\n",
    "print(\"Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\")\n",
    "\n",
    "for epoch in range(EPOCHS_DE_PRUEBA):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # ---- Train ----\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validaci贸n ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_one_hot in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels_one_hot = labels_one_hot.to(DEVICE)\n",
    "\n",
    "            labels_indices = torch.argmax(labels_one_hot, dim=1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels_indices)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels_indices.size(0)\n",
    "            correct += (predicted == labels_indices).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_DE_PRUEBA} - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Entrenamiento de la cabeza finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e909a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en el set de validaci贸n...\n",
      "\\n==========================================================\n",
      " 隆Prueba de Evaluaci贸n Lineal (Linear Probing - ResNet50 Baseline) completa! \n",
      "   Accuracy en el set de test: 98.22 %\n",
      "   F1 Score (Macro) en el set de test: 98.22 %\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- 6. EVALUAR EL RENDIMIENTO CON F1 SCORE ---\n",
    "\n",
    "print(\"Evaluando en el set de validaci贸n...\")\n",
    "\n",
    "# Lista para almacenar todas las etiquetas verdaderas y predichas\n",
    "all_labels = []\n",
    "all_predicted = []\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_one_hot in test_loader:\n",
    "        # Mover datos al dispositivo (CPU/GPU)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels_one_hot = labels_one_hot.to(DEVICE)\n",
    "\n",
    "        labels_indices = torch.argmax(labels_one_hot, dim=1)\n",
    "        \n",
    "        # 1. Pase adelante (Forward Pass)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2. Obtener la predicci贸n de clase\n",
    "        _, predicted_indices = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 3. Almacenar para el c谩lculo de F1 Score\n",
    "        # Mover a CPU para Scikit-learn y convertir a numpy\n",
    "        all_labels.extend(labels_indices.cpu().numpy())\n",
    "        all_predicted.extend(predicted_indices.cpu().numpy())\n",
    "        \n",
    "        # 4. Actualizar contadores de Accuracy\n",
    "        total += labels_one_hot.size(0)\n",
    "        correct += (predicted_indices == labels_indices).sum().item()\n",
    "\n",
    "\n",
    "# --- CLCULO DE MTRICAS ---\n",
    "\n",
    "# 1. Calcular Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# 2. Calcular F1 Score\n",
    "# 'average=\"macro\"' se usa com煤nmente en problemas multi-clase para dar\n",
    "# igual peso a cada clase, independientemente del desequilibrio.\n",
    "# Cambiar a 'average=\"weighted\"' si se necesita considerar el desequilibrio de clases.\n",
    "f1 = f1_score(all_labels, all_predicted, average='macro') \n",
    "f1_percentage = f1 * 100\n",
    "\n",
    "# --- RESULTADO FINAL ---\n",
    "print(\"\\\\n==========================================================\") \n",
    "print(f\" 隆Prueba de Evaluaci贸n Lineal (Linear Probing - ResNet50 Baseline) completa! \")\n",
    "print(f\"   Accuracy en el set de test: {accuracy:.2f} %\") \n",
    "print(f\"   F1 Score (Macro) en el set de test: {f1_percentage:.2f} %\") \n",
    "print(\"==========================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
