{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ce8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "# Import Dataset to inherit from it\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import for the demonstration code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "# --- End PyTorch Imports ---\n",
    "\n",
    "class ImageDatasetWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset compatible con PyTorch que escanea subdirectorios de clases.\n",
    "    Hereda de torch.utils.data.Dataset.\n",
    "    Devuelve etiquetas como vectores one-hot (np.ndarray).\n",
    "    \n",
    "    隆NUEVO! Tambi茅n crea una lista 'self.targets' con etiquetas enteras\n",
    "    (ej. 0, 1, 2) para ser usada por 'sklearn.model_selection.train_test_split'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, transform: Any = None):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset, escanea el directorio y crea el mapa de 铆ndices.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # data_index almacenar谩 (filepath, one_hot_label)\n",
    "        self.data_index: List[Tuple[str, np.ndarray]] = []\n",
    "        \n",
    "        # --- 隆CORRECCIN AADIDA AQU! ---\n",
    "        # self.targets almacenar谩 el 铆ndice entero (0, 1, 2...) para la estratificaci贸n\n",
    "        self.targets: List[int] = []\n",
    "        # --- FIN DE LA CORRECCIN ---\n",
    "        \n",
    "        self.class_names: List[str] = []\n",
    "        self.class_to_label: Dict[str, np.ndarray] = {}\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"\n",
    "        Escanea el directorio ra铆z en busca de carpetas de clases y rellena \n",
    "        data_index (para los datos) y targets (para la divisi贸n).\n",
    "        \"\"\"\n",
    "        print(f\"Escaneando directorio: {self.root_dir}\")\n",
    "\n",
    "        # 1. Descubrir nombres de clases (subdirectorios)\n",
    "        subdirs = [d for d in os.listdir(self.root_dir)\n",
    "                   if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        self.class_names = sorted(subdirs)\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        if num_classes == 0:\n",
    "            raise ValueError(f\"No se encontraron subdirectorios de clases en {self.root_dir}\")\n",
    "\n",
    "        # 2. Crear mapeo class_to_label (para arrays one-hot)\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            one_hot = np.zeros(num_classes, dtype=np.float32)\n",
    "            one_hot[i] = 1.0\n",
    "            self.class_to_label[class_name] = one_hot\n",
    "\n",
    "        print(f\"Se encontraron {num_classes} clases: {self.class_names}\")\n",
    "\n",
    "        # 3. Rellenar la lista de 铆ndices maestros\n",
    "        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "        for class_index, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            one_hot_label = self.class_to_label[class_name]\n",
    "\n",
    "            # Listar archivos en el directorio de la clase\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.lower().endswith(image_extensions):\n",
    "                    filepath = os.path.join(class_path, filename)\n",
    "                    # Almacenar (filepath, one_hot_label)\n",
    "                    self.data_index.append((filepath, one_hot_label))\n",
    "                    \n",
    "                    # --- 隆CORRECCIN AADIDA AQU! ---\n",
    "                    # Almacenar el 铆ndice entero (0, 1, 2...)\n",
    "                    self.targets.append(class_index)\n",
    "                    # --- FIN DE LA CORRECCIN ---\n",
    "\n",
    "        print(f\"Total de im谩genes indexadas: {len(self.data_index)}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Devuelve el n煤mero total de items (im谩genes) en el dataset.\"\"\"\n",
    "        return len(self.data_index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Recupera la imagen y su etiqueta one-hot correspondiente.\n",
    "        Aplica transformaciones si se proporcionan.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.data_index) or idx < 0:\n",
    "            raise IndexError(\"ndice fuera de rango\")\n",
    "\n",
    "        filepath, label_vector = self.data_index[idx]\n",
    "\n",
    "        # 1. Cargar la imagen con PIL\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar la imagen {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 2. Aplicar transformaciones (ej. ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Devuelve la imagen transformada y el vector one-hot\n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0776e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el dataset completo para indexar...\n",
      "Escaneando directorio: /lustre/proyectos/p032/datasets/images/3kvasir\n",
      "Se encontraron 3 clases: ['normal-cecum', 'normal-pylorus', 'normal-z-line']\n",
      "Total de im谩genes indexadas: 1500\n",
      "Total de im谩genes encontradas: 1500\n",
      "Realizando primera divisi贸n (estratificada)...\n",
      "Realizando segunda divisi贸n (estratificada)...\n",
      "\n",
      "--- 隆Divisi贸n completada! ---\n",
      "Total:      1500\n",
      "Set Train:  1049\n",
      "Set Val:    226\n",
      "Set Test:   225\n",
      "\n",
      "DataLoaders estratificados (train, val, test) creados.\n",
      "\n",
      "Verificando distribuci贸n (ejemplo):\n",
      "  Train: ['33.37%', '33.37%', '33.27%']\n",
      "  Val:   ['33.19%', '33.19%', '33.63%']\n",
      "  Test:  ['33.33%', '33.33%', '33.33%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Un NUEVO Dataset Wrapper (m谩s simple)\n",
    "# ---------------------------------------------------------------\n",
    "class PreSplitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset que acepta una lista de datos (filepath, label) \n",
    "    pre-dividida en su constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list: List[Tuple[str, np.ndarray]], transform: Any = None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Obtener el filepath y la etiqueta de la lista\n",
    "        filepath, label_vector = self.data_list[idx]\n",
    "\n",
    "        # Cargar la imagen\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            raise\n",
    "            \n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label_vector\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Configuraci贸n y Proceso de Divisi贸n\n",
    "# ---------------------------------------------------------------\n",
    "# --- Configuraci贸n ---\n",
    "dataset_root = \"/lustre/proyectos/p032/datasets/images/3kvasir\"\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# Definir los ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15 # (debe sumar 1.0)\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 1. Cargar el dataset COMPLETO ---\n",
    "print(\"Cargando el dataset completo para indexar...\")\n",
    "# (Necesitamos la clase 'ImageDatasetWrapper' original para esto)\n",
    "# (He a帽adido .targets a la clase para que esto funcione)\n",
    "full_dataset = ImageDatasetWrapper(root_dir=dataset_root)\n",
    "\n",
    "# Extraer los datos y las etiquetas para sklearn\n",
    "# data_index es List[Tuple[str, np.ndarray]]\n",
    "# targets es List[int] (ej. 0, 1, 2, 0, 1...)\n",
    "all_data = full_dataset.data_index \n",
    "all_targets = full_dataset.targets \n",
    "\n",
    "if len(all_data) == 0:\n",
    "    raise RuntimeError(\"Error: No se encontraron datos en el dataset.\")\n",
    "\n",
    "print(f\"Total de im谩genes encontradas: {len(all_data)}\")\n",
    "\n",
    "# --- 2. Primera Divisi贸n (Train+Val vs Test) ---\n",
    "# Dividimos el 85% para (train+val) y el 15% para test\n",
    "print(\"Realizando primera divisi贸n (estratificada)...\")\n",
    "train_val_data, test_data, train_val_targets, test_targets = train_test_split(\n",
    "    all_data,\n",
    "    all_targets,\n",
    "    test_size=TEST_RATIO,\n",
    "    stratify=all_targets, # 隆La clave es esta!\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# --- 3. Segunda Divisi贸n (Train vs Val) ---\n",
    "# Dividimos (train+val) en train y val\n",
    "# El ratio debe recalcularse: VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "val_split_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "\n",
    "print(\"Realizando segunda divisi贸n (estratificada)...\")\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(\n",
    "    train_val_data,\n",
    "    train_val_targets,\n",
    "    test_size=val_split_ratio,\n",
    "    stratify=train_val_targets, # Estratificar de nuevo\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n--- 隆Divisi贸n completada! ---\")\n",
    "print(f\"Total:      {len(all_data)}\")\n",
    "print(f\"Set Train:  {len(train_data)}\")\n",
    "print(f\"Set Val:    {len(val_data)}\")\n",
    "print(f\"Set Test:   {len(test_data)}\")\n",
    "\n",
    "# --- 4. Crear los Datasets y DataLoaders ---\n",
    "\n",
    "# Aplicar la transformaci贸n a cada set\n",
    "train_dataset = PreSplitDataset(train_data, transform=transform)\n",
    "val_dataset = PreSplitDataset(val_data, transform=transform)\n",
    "test_dataset = PreSplitDataset(test_data, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"\\nDataLoaders estratificados (train, val, test) creados.\")\n",
    "\n",
    "# --- 5. (Opcional) Verificar la distribuci贸n de clases ---\n",
    "print(\"\\nVerificando distribuci贸n (ejemplo):\")\n",
    "\n",
    "def get_class_counts(targets_list):\n",
    "    counts = np.bincount(targets_list)\n",
    "    return [f\"{count/len(targets_list)*100:.2f}%\" for count in counts]\n",
    "    \n",
    "print(f\"  Train: {get_class_counts(train_targets)}\")\n",
    "print(f\"  Val:   {get_class_counts(val_targets)}\")\n",
    "print(f\"  Test:  {get_class_counts(test_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7daae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "N煤mero de clases: 3\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURACIN INICIAL ---\n",
    "# ==========================================================\n",
    "# Coloca aqu铆 la ruta a tu modelo .pth (el checkpoint de Lightning)\n",
    "PATH_MODELO_SSL = \"/lustre/proyectos/p032/models/multi_pretext_model2.ckpt\"\n",
    "MODEL_PATH = \"/lustre/home/opacheco/MEDA_Challenge/models/221025MG_backbone.ssl.pth\"\n",
    "\n",
    "# 驴Cu谩ntas clases tiene tu dataset de PRUEBA?\n",
    "NUM_CLASES = 3\n",
    "\n",
    "# Par谩metros\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_DE_PRUEBA = 10\n",
    "LEARNING_RATE = 0.001\n",
    "JIGSAW_N = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "print(f\"N煤mero de clases: {NUM_CLASES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a027fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Demonstrates the usage of the new PyTorch-compatible Dataset.\"\"\"\n",
    "# dataset_root = \"/lustre/proyectos/p032/datasets/images/3kvasir\"\n",
    "\n",
    "# # 2. Define a transform pipeline (similar to your example)\n",
    "# # This now handles all resizing, cropping, and tensor conversion\n",
    "# demo_transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(), # Converts PIL Image to torch.Tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                             std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # 3. Initialize the wrapper, passing the transform\n",
    "# try:\n",
    "#     dataset = ImageDatasetWrapper(dataset_root, transform=demo_transform)\n",
    "\n",
    "#     # 4. Test __len__\n",
    "#     print(f\"Dataset length (__len__): {len(dataset)}\")\n",
    "\n",
    "#     # 5. Test __getitem__ (e.g., get the 4th item)\n",
    "#     test_index = 3 \n",
    "\n",
    "#     image_data, label_vector = dataset[test_index]\n",
    "    \n",
    "#     # 6. Print results\n",
    "#     print(f\"\\n--- Item at Index {test_index} ---\")\n",
    "    \n",
    "#     # Updated checks for PyTorch Tensor\n",
    "#     print(f\"Image type: {type(image_data)}\")\n",
    "#     print(f\"Image shape: {image_data.shape} (C, H, W) -> Is now a Tensor.\")\n",
    "    \n",
    "#     print(f\"Label type: {type(label_vector)}\")\n",
    "#     print(f\"Label value: {label_vector}\") # This is now a numpy array\n",
    "    \n",
    "#     # Check which class the label vector corresponds to\n",
    "#     class_index = np.argmax(label_vector)\n",
    "#     predicted_class = dataset.class_names[class_index]\n",
    "#     print(f\"Corresponding Class Name: {predicted_class}\")\n",
    "    \n",
    "#     # You could now pass this 'dataset' object directly to a DataLoader\n",
    "#     # from torch.utils.data import DataLoader\n",
    "#     loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "#     print(\"\\nSuccessfully created DataLoader.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nAn error occurred during demonstration: {e}\")\n",
    "#     print(\"Please ensure you have PyTorch and Torchvision installed (`pip install torch torchvision`) to run the demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3057291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Modelo Multi-Pretexto (Versi贸n LightningModule) ---\n",
    "\n",
    "class MultiPretextSSL_Lightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate') # Guarda lr\n",
    "        self.backbone = backbone\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        num_features = 512 # Salida de ResNet18\n",
    "        \n",
    "        # --- DECODER CORREGIDO PARA 28x28 ---\n",
    "        decoder_layers_28x28 = [\n",
    "            nn.ConvTranspose2d(num_features, 256, kernel_size=4, stride=1, padding=0), # 1x1 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1), # 4x4 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),    # 14x14 -> 28x28\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        self.color_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        self.patch_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        \n",
    "        # --- JIGSAW HEAD CORREGIDO PARA N=4 (16 patches) ---\n",
    "        self.n_patches = JIGSAW_N * JIGSAW_N # 16\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_patches * self.n_patches) # 16*16 = 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x, task=\"color\"):\n",
    "        feats = self.backbone(x)\n",
    "        if task == \"color\":\n",
    "            return self.color_head(feats)\n",
    "        elif task == \"patch\":\n",
    "            return self.patch_head(feats)\n",
    "        elif task == \"jigsaw\":\n",
    "            return self.jigsaw_head(feats)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch es lo que retorna __getitem__, en este caso, 'imgs'\n",
    "        imgs = batch\n",
    "        \n",
    "        # Elegir una tarea al azar\n",
    "        task = random.choice([\"color\", \"patch\", \"jigsaw\"])\n",
    "        loss = 0.0\n",
    "\n",
    "        if task == \"color\":\n",
    "            inp, target = colorization_pair_tensor(imgs)\n",
    "            pred = self(inp, \"color\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"patch\":\n",
    "            inp, target = patch_prediction_pair_tensor(imgs)\n",
    "            pred = self(inp, \"patch\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"jigsaw\":\n",
    "            inp, target = jigsaw_pair_tensor(imgs, n=JIGSAW_N) # n=4\n",
    "            pred = self(inp, \"jigsaw\") # Shape: [B, 256]\n",
    "            \n",
    "            # [B, 256] -> [B, 16, 16]\n",
    "            pred_reshaped = pred.view(-1, self.n_patches, self.n_patches) \n",
    "            target_reshaped = target.view(-1) # [B*16]\n",
    "            \n",
    "            loss = F.cross_entropy(pred_reshaped.view(-1, self.n_patches), target_reshaped)\n",
    "\n",
    "        # Loggear la p茅rdida. 'prog_bar=True' la muestra en la barra de progreso\n",
    "        self.log(f'loss_{task}', loss, prog_bar=True)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b554d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCoLightning(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder_q = nn.Sequential(backbone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4b295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone cargado y movido a GPU.\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(weights=None) \n",
    "# Tu backbone (quitando la capa FC final)\n",
    "backbone_structure = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Cargar el estado\n",
    "encoder_wrapper = MoCoLightning(backbone=backbone_structure)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Cargar los pesos en la estructura\n",
    "encoder_wrapper.encoder_q[0].load_state_dict(state_dict)\n",
    "\n",
    "# --- Este es tu backbone listo para usar ---\n",
    "ssl_backbone = encoder_wrapper.encoder_q[0].to(DEVICE)\n",
    "print(\"Backbone cargado y movido a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6333c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiPretextSSL_Lightning.load_from_checkpoint(PATH_MODELO_SSL, backbone=ssl_backbone)\n",
    "\n",
    "# Congelar todo el backbone\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = 512  # ResNet18 sin FC tiene 512 features\n",
    "# Crear la cabeza lineal\n",
    "linear_head = nn.Linear(in_features, NUM_CLASES)\n",
    "\n",
    "class LinearProbingModel(nn.Module):\n",
    "    def __init__(self, backbone, linear_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.linear_head = linear_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # [B, 512, 1, 1]\n",
    "        feats = feats.view(feats.size(0), -1)  # Flatten -> [B, 512]\n",
    "        out = self.linear_head(feats)     # [B, NUM_CLASES]\n",
    "        return out\n",
    "\n",
    "model = LinearProbingModel(model.backbone, linear_head).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.linear_head.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c731719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\n",
      "Epoch 1/10 - Train Loss: 0.8432 - Val Loss: 0.7748 - Val Acc: 73.01%\n",
      "Epoch 2/10 - Train Loss: 0.5428 - Val Loss: 0.4898 - Val Acc: 87.17%\n",
      "Epoch 3/10 - Train Loss: 0.4298 - Val Loss: 0.4323 - Val Acc: 84.07%\n",
      "Epoch 4/10 - Train Loss: 0.3882 - Val Loss: 0.3678 - Val Acc: 87.61%\n",
      "Epoch 5/10 - Train Loss: 0.3490 - Val Loss: 0.3420 - Val Acc: 88.94%\n",
      "Epoch 6/10 - Train Loss: 0.3256 - Val Loss: 0.3499 - Val Acc: 89.38%\n",
      "Epoch 7/10 - Train Loss: 0.3246 - Val Loss: 0.3099 - Val Acc: 91.59%\n",
      "Epoch 8/10 - Train Loss: 0.3240 - Val Loss: 0.3250 - Val Acc: 90.27%\n",
      "Epoch 9/10 - Train Loss: 0.3087 - Val Loss: 0.2980 - Val Acc: 91.15%\n",
      "Epoch 10/10 - Train Loss: 0.3108 - Val Loss: 0.2805 - Val Acc: 91.15%\n",
      "Entrenamiento de la cabeza finalizado.\n"
     ]
    }
   ],
   "source": [
    "# --- ENTRENAR LA CABEZA LINEAL CON VALIDACIN ---\n",
    "\n",
    "print(\"Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\")\n",
    "\n",
    "for epoch in range(EPOCHS_DE_PRUEBA):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # ---- Train ----\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validaci贸n ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_one_hot in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels_one_hot = labels_one_hot.to(DEVICE)\n",
    "\n",
    "            labels_indices = torch.argmax(labels_one_hot, dim=1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels_indices)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels_indices.size(0)\n",
    "            correct += (predicted == labels_indices).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_DE_PRUEBA} - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Entrenamiento de la cabeza finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad5244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en el set de validaci贸n...\n",
      "\n",
      "========================================================\n",
      " 隆Prueba de Evaluaci贸n Lineal (Linear Probing) completa! \n",
      "   Accuracy en el set de validaci贸n: 89.78 %\n",
      "   F1 Score (Macro) en el set de validaci贸n: 89.69 %\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- 6. EVALUAR EL RENDIMIENTO CON F1 SCORE ---\n",
    "\n",
    "print(\"Evaluando en el set de validaci贸n...\")\n",
    "\n",
    "# Lista para almacenar todas las etiquetas verdaderas y predichas\n",
    "all_labels = []\n",
    "all_predicted = []\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_one_hot in test_loader:\n",
    "        # Mover datos al dispositivo (CPU/GPU)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels_one_hot = labels_one_hot.to(DEVICE)\n",
    "\n",
    "        labels_indices = torch.argmax(labels_one_hot, dim=1)\n",
    "        \n",
    "        # 1. Pase adelante (Forward Pass)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2. Obtener la predicci贸n de clase\n",
    "        _, predicted_indices = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 3. Almacenar para el c谩lculo de F1 Score\n",
    "        # Mover a CPU para Scikit-learn y convertir a numpy\n",
    "        all_labels.extend(labels_indices.cpu().numpy())\n",
    "        all_predicted.extend(predicted_indices.cpu().numpy())\n",
    "        \n",
    "        # 4. Actualizar contadores de Accuracy\n",
    "        total += labels_one_hot.size(0)\n",
    "        correct += (predicted_indices == labels_indices).sum().item()\n",
    "\n",
    "\n",
    "# --- CLCULO DE MTRICAS ---\n",
    "\n",
    "# 1. Calcular Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# 2. Calcular F1 Score\n",
    "# 'average=\"macro\"' se usa com煤nmente en problemas multi-clase para dar\n",
    "# igual peso a cada clase, independientemente del desequilibrio.\n",
    "# Cambiar a 'average=\"weighted\"' si se necesita considerar el desequilibrio de clases.\n",
    "f1 = f1_score(all_labels, all_predicted, average='macro') \n",
    "f1_percentage = f1 * 100\n",
    "\n",
    "# --- RESULTADO FINAL ---\n",
    "print(\"\\n========================================================\")\n",
    "print(f\" 隆Prueba de Evaluaci贸n Lineal (Linear Probing) completa! \")\n",
    "print(f\"   Accuracy en el set de validaci贸n: {accuracy:.2f} %\")\n",
    "print(f\"   F1 Score (Macro) en el set de validaci贸n: {f1_percentage:.2f} %\")\n",
    "print(\"========================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
