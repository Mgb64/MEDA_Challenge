{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f102c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930201fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Dataset\n",
      "Dataset Cargado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scripts.SSL import MoCoLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d090ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # Quitar la capa final\n",
    "\n",
    "encoder = MoCoLightning(\n",
    "        backbone=backbone,\n",
    "        lr=0.0003,          # El LR que tenías\n",
    "        temperature=0.1,    # La temperatura que tenías\n",
    "        queue_size=8192     # Un valor más pequeño si 65536 da OOM\n",
    "    )\n",
    "\n",
    "\n",
    "state_dict = torch.load(\"/lustre/home/atorres/MEDA_Challenge/models/221025MG_backbone.ssl.pth\", map_location='cuda')\n",
    "\n",
    "# Como guardaste solo encoder_q[0], necesitas asignarlo a esa parte del modelo\n",
    "encoder.encoder_q[0].load_state_dict(state_dict)\n",
    "\n",
    "encoder = encoder.cuda() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d906cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26266700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedMNISTUnifiedFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.files = [os.path.join(root, f) for f in os.listdir(root)\n",
    "                      if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c55df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((28,28)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = MedMNISTUnifiedFolder(\"/lustre/home/atorres/compartido/datasets/all_medmnist_images\", transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cdfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorization_pair(img):\n",
    "    gray = T.Grayscale()(img)\n",
    "    return gray, img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ce9a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigsaw: simplificación, target = primer índice de permutación\n",
    "def jigsaw_pair(imgs, n=3):\n",
    "    B, C, H, W = imgs.shape\n",
    "    shuffled_imgs = []\n",
    "    orders = []\n",
    "    for i in range(B):\n",
    "        patches = []\n",
    "        for y in range(n):\n",
    "            for x in range(n):\n",
    "                patch_h, patch_w = H // n, W // n\n",
    "                patch = imgs[i, :, y*patch_h:(y+1)*patch_h, x*patch_w:(x+1)*patch_w]\n",
    "                patches.append(patch)\n",
    "        order = torch.randperm(len(patches))\n",
    "        shuffled = torch.zeros_like(imgs[i])\n",
    "        for k, idx in enumerate(order):\n",
    "            yk, xk = divmod(k, n)\n",
    "            shuffled[:, yk*patch_h:(yk+1)*patch_h, xk*patch_w:(xk+1)*patch_w] = patches[idx]\n",
    "        shuffled_imgs.append(shuffled)\n",
    "        orders.append(order[0])  # solo el primer índice para simplificar\n",
    "    return torch.stack(shuffled_imgs), torch.tensor(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77dd6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def patch_prediction_pair(imgs, mask_size=16):\n",
    "    # imgs: tensor BxCxHxW\n",
    "    masked_imgs = []\n",
    "    target_imgs = []\n",
    "\n",
    "    for img in imgs:\n",
    "        # Convertir tensor a PIL\n",
    "        pil_img = TF.to_pil_image(img.cpu())\n",
    "        w, h = pil_img.size\n",
    "        x = (w - mask_size) // 2\n",
    "        y = (h - mask_size) // 2\n",
    "\n",
    "        masked = pil_img.copy()\n",
    "        draw = ImageDraw.Draw(masked)\n",
    "        draw.rectangle([x, y, x+mask_size, y+mask_size], fill=(0,0,0))\n",
    "\n",
    "        # Convertir de vuelta a tensor\n",
    "        masked_tensor = TF.to_tensor(masked).to(img.device)\n",
    "        target_tensor = img\n",
    "\n",
    "        masked_imgs.append(masked_tensor)\n",
    "        target_imgs.append(target_tensor)\n",
    "\n",
    "    return torch.stack(masked_imgs), torch.stack(target_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9787ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiPretextSSL(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone  # tu encoder SSL\n",
    "\n",
    "        # Cada tarea tiene su propia \"head\"\n",
    "        self.color_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.jigsaw_head = nn.Linear(512, 9 * 9)  # predicción del orden 9x9\n",
    "        self.patch_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, task=\"color\"):\n",
    "        feats = self.backbone(x).squeeze()\n",
    "        if task == \"color\":\n",
    "            return self.color_head(feats.unsqueeze(-1).unsqueeze(-1))\n",
    "        elif task == \"patch\":\n",
    "            return self.patch_head(feats.unsqueeze(-1).unsqueeze(-1))\n",
    "        elif task == \"jigsaw\":\n",
    "            return self.jigsaw_head(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604da6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     shuffled, target_order \u001b[38;5;241m=\u001b[39m jigsaw_pair(imgs)\n\u001b[1;32m     21\u001b[0m     shuffled, target_order \u001b[38;5;241m=\u001b[39m shuffled\u001b[38;5;241m.\u001b[39mcuda(), target_order\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshuffled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjigsaw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred, target_order)\n\u001b[1;32m     25\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Bucle de entrenamiento multitask ---\n",
    "for epoch in range(10):\n",
    "    for imgs in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        task = random.choice([\"color\", \"patch\", \"jigsaw\"])\n",
    "\n",
    "        if task == \"color\":\n",
    "            gray, target = colorization_pair(imgs)\n",
    "            gray, target = gray.cuda(), target.cuda()\n",
    "            pred = encoder(gray, \"color\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"patch\":\n",
    "            masked, target = patch_prediction_pair(imgs)\n",
    "            masked, target = masked.cuda(), target.cuda()\n",
    "            pred = encoder(masked, \"patch\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        else:  # jigsaw\n",
    "            shuffled, target_order = jigsaw_pair(imgs)\n",
    "            shuffled, target_order = shuffled.cuda(), target_order.cuda()\n",
    "            pred = encoder(shuffled, \"jigsaw\")\n",
    "            loss = F.cross_entropy(pred, target_order)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Task: {task} | Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
