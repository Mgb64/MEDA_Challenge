{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "# Import Dataset to inherit from it\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import for the demonstration code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "# --- End PyTorch Imports ---\n",
    "\n",
    "class ImageDatasetWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset compatible con PyTorch que escanea subdirectorios de clases.\n",
    "    Hereda de torch.utils.data.Dataset.\n",
    "    Devuelve etiquetas como vectores one-hot (np.ndarray).\n",
    "    \n",
    "    ¡NUEVO! También crea una lista 'self.targets' con etiquetas enteras\n",
    "    (ej. 0, 1, 2) para ser usada por 'sklearn.model_selection.train_test_split'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, transform: Any = None):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset, escanea el directorio y crea el mapa de índices.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # data_index almacenará (filepath, one_hot_label)\n",
    "        self.data_index: List[Tuple[str, np.ndarray]] = []\n",
    "        \n",
    "        # --- ¡CORRECCIÓN AÑADIDA AQUÍ! ---\n",
    "        # self.targets almacenará el índice entero (0, 1, 2...) para la estratificación\n",
    "        self.targets: List[int] = []\n",
    "        # --- FIN DE LA CORRECCIÓN ---\n",
    "        \n",
    "        self.class_names: List[str] = []\n",
    "        self.class_to_label: Dict[str, np.ndarray] = {}\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"\n",
    "        Escanea el directorio raíz en busca de carpetas de clases y rellena \n",
    "        data_index (para los datos) y targets (para la división).\n",
    "        \"\"\"\n",
    "        print(f\"Escaneando directorio: {self.root_dir}\")\n",
    "\n",
    "        # 1. Descubrir nombres de clases (subdirectorios)\n",
    "        subdirs = [d for d in os.listdir(self.root_dir)\n",
    "                   if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        self.class_names = sorted(subdirs)\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        if num_classes == 0:\n",
    "            raise ValueError(f\"No se encontraron subdirectorios de clases en {self.root_dir}\")\n",
    "\n",
    "        # 2. Crear mapeo class_to_label (para arrays one-hot)\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            one_hot = np.zeros(num_classes, dtype=np.float32)\n",
    "            one_hot[i] = 1.0\n",
    "            self.class_to_label[class_name] = one_hot\n",
    "\n",
    "        print(f\"Se encontraron {num_classes} clases: {self.class_names}\")\n",
    "\n",
    "        # 3. Rellenar la lista de índices maestros\n",
    "        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "        for class_index, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            one_hot_label = self.class_to_label[class_name]\n",
    "\n",
    "            # Listar archivos en el directorio de la clase\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.lower().endswith(image_extensions):\n",
    "                    filepath = os.path.join(class_path, filename)\n",
    "                    # Almacenar (filepath, one_hot_label)\n",
    "                    self.data_index.append((filepath, one_hot_label))\n",
    "                    \n",
    "                    # --- ¡CORRECCIÓN AÑADIDA AQUÍ! ---\n",
    "                    # Almacenar el índice entero (0, 1, 2...)\n",
    "                    self.targets.append(class_index)\n",
    "                    # --- FIN DE LA CORRECCIÓN ---\n",
    "\n",
    "        print(f\"Total de imágenes indexadas: {len(self.data_index)}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Devuelve el número total de items (imágenes) en el dataset.\"\"\"\n",
    "        return len(self.data_index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Recupera la imagen y su etiqueta one-hot correspondiente.\n",
    "        Aplica transformaciones si se proporcionan.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.data_index) or idx < 0:\n",
    "            raise IndexError(\"Índice fuera de rango\")\n",
    "\n",
    "        filepath, label_vector = self.data_index[idx]\n",
    "\n",
    "        # 1. Cargar la imagen con PIL\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar la imagen {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 2. Aplicar transformaciones (ej. ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Devuelve la imagen transformada y el vector one-hot\n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el dataset completo para indexar...\n",
      "Escaneando directorio: /lustre/proyectos/p032/datasets/images/3kvasir\n",
      "Se encontraron 3 clases: ['normal-cecum', 'normal-pylorus', 'normal-z-line']\n",
      "Total de imágenes indexadas: 1500\n",
      "Total de imágenes encontradas: 1500\n",
      "Realizando primera división (estratificada)...\n",
      "Realizando segunda división (estratificada)...\n",
      "\n",
      "--- ¡División completada! ---\n",
      "Total:      1500\n",
      "Set Train:  1049\n",
      "Set Val:    226\n",
      "Set Test:   225\n",
      "\n",
      "DataLoaders estratificados (train, val, test) creados.\n",
      "\n",
      "Verificando distribución (ejemplo):\n",
      "  Train: ['33.37%', '33.37%', '33.27%']\n",
      "  Val:   ['33.19%', '33.19%', '33.63%']\n",
      "  Test:  ['33.33%', '33.33%', '33.33%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Un NUEVO Dataset Wrapper (más simple)\n",
    "# ---------------------------------------------------------------\n",
    "class PreSplitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset que acepta una lista de datos (filepath, label) \n",
    "    pre-dividida en su constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list: List[Tuple[str, np.ndarray]], transform: Any = None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Obtener el filepath y la etiqueta de la lista\n",
    "        filepath, label_vector = self.data_list[idx]\n",
    "\n",
    "        # Cargar la imagen\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            raise\n",
    "            \n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label_vector\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Configuración y Proceso de División\n",
    "# ---------------------------------------------------------------\n",
    "# --- Configuración ---\n",
    "dataset_root = \"/lustre/proyectos/p032/datasets/images/3kvasir\"\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# Definir los ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15 # (debe sumar 1.0)\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 1. Cargar el dataset COMPLETO ---\n",
    "print(\"Cargando el dataset completo para indexar...\")\n",
    "# (Necesitamos la clase 'ImageDatasetWrapper' original para esto)\n",
    "# (He añadido .targets a la clase para que esto funcione)\n",
    "full_dataset = ImageDatasetWrapper(root_dir=dataset_root)\n",
    "\n",
    "# Extraer los datos y las etiquetas para sklearn\n",
    "# data_index es List[Tuple[str, np.ndarray]]\n",
    "# targets es List[int] (ej. 0, 1, 2, 0, 1...)\n",
    "all_data = full_dataset.data_index \n",
    "all_targets = full_dataset.targets \n",
    "\n",
    "if len(all_data) == 0:\n",
    "    raise RuntimeError(\"Error: No se encontraron datos en el dataset.\")\n",
    "\n",
    "print(f\"Total de imágenes encontradas: {len(all_data)}\")\n",
    "\n",
    "# --- 2. Primera División (Train+Val vs Test) ---\n",
    "# Dividimos el 85% para (train+val) y el 15% para test\n",
    "print(\"Realizando primera división (estratificada)...\")\n",
    "train_val_data, test_data, train_val_targets, test_targets = train_test_split(\n",
    "    all_data,\n",
    "    all_targets,\n",
    "    test_size=TEST_RATIO,\n",
    "    stratify=all_targets, # ¡La clave es esta!\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# --- 3. Segunda División (Train vs Val) ---\n",
    "# Dividimos (train+val) en train y val\n",
    "# El ratio debe recalcularse: VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "val_split_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "\n",
    "print(\"Realizando segunda división (estratificada)...\")\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(\n",
    "    train_val_data,\n",
    "    train_val_targets,\n",
    "    test_size=val_split_ratio,\n",
    "    stratify=train_val_targets, # Estratificar de nuevo\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n--- ¡División completada! ---\")\n",
    "print(f\"Total:      {len(all_data)}\")\n",
    "print(f\"Set Train:  {len(train_data)}\")\n",
    "print(f\"Set Val:    {len(val_data)}\")\n",
    "print(f\"Set Test:   {len(test_data)}\")\n",
    "\n",
    "# --- 4. Crear los Datasets y DataLoaders ---\n",
    "\n",
    "# Aplicar la transformación a cada set\n",
    "train_dataset = PreSplitDataset(train_data, transform=transform)\n",
    "val_dataset = PreSplitDataset(val_data, transform=transform)\n",
    "test_dataset = PreSplitDataset(test_data, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"\\nDataLoaders estratificados (train, val, test) creados.\")\n",
    "\n",
    "# --- 5. (Opcional) Verificar la distribución de clases ---\n",
    "print(\"\\nVerificando distribución (ejemplo):\")\n",
    "\n",
    "def get_class_counts(targets_list):\n",
    "    counts = np.bincount(targets_list)\n",
    "    return [f\"{count/len(targets_list)*100:.2f}%\" for count in counts]\n",
    "    \n",
    "print(f\"  Train: {get_class_counts(train_targets)}\")\n",
    "print(f\"  Val:   {get_class_counts(val_targets)}\")\n",
    "print(f\"  Test:  {get_class_counts(test_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURACIÓN INICIAL ---\n",
    "# ==========================================================\n",
    "# PATH_MODELO_SSL = \"/lustre/proyectos/p032/models/multi_pretext_model2.ckpt\" # No se usa\n",
    "# MODEL_PATH = \"/lustre/home/opacheco/MEDA_Challenge/models/221025MG_backbone.ssl.pth\" # No se usa\n",
    "\n",
    "# ¿Cuántas clases tiene tu dataset de PRUEBA?\n",
    "NUM_CLASES = 3 # Esto sigue siendo correcto para tu 3kvasir\n",
    "\n",
    "# Parámetros (¡Importante usar los mismos!)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_DE_PRUEBA = 10\n",
    "LEARNING_RATE = 0.001 # Este LR se usó para el Linear Probe, ¡mantenerlo!\n",
    "# JIGSAW_N = 2 # No aplica aquí\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "print(f\"Número de clases: {NUM_CLASES}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
