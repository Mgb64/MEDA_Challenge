{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0d96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "# Import Dataset to inherit from it\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import for the demonstration code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "# --- End PyTorch Imports ---\n",
    "\n",
    "class ImageDatasetWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset compatible con PyTorch que escanea subdirectorios de clases.\n",
    "    Hereda de torch.utils.data.Dataset.\n",
    "    Devuelve etiquetas como vectores one-hot (np.ndarray).\n",
    "    \n",
    "    ¬°NUEVO! Tambi√©n crea una lista 'self.targets' con etiquetas enteras\n",
    "    (ej. 0, 1, 2) para ser usada por 'sklearn.model_selection.train_test_split'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, transform: Any = None):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset, escanea el directorio y crea el mapa de √≠ndices.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # data_index almacenar√° (filepath, one_hot_label)\n",
    "        self.data_index: List[Tuple[str, np.ndarray]] = []\n",
    "        \n",
    "        # --- ¬°CORRECCI√ìN A√ëADIDA AQU√ç! ---\n",
    "        # self.targets almacenar√° el √≠ndice entero (0, 1, 2...) para la estratificaci√≥n\n",
    "        self.targets: List[int] = []\n",
    "        # --- FIN DE LA CORRECCI√ìN ---\n",
    "        \n",
    "        self.class_names: List[str] = []\n",
    "        self.class_to_label: Dict[str, np.ndarray] = {}\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"\n",
    "        Escanea el directorio ra√≠z en busca de carpetas de clases y rellena \n",
    "        data_index (para los datos) y targets (para la divisi√≥n).\n",
    "        \"\"\"\n",
    "        print(f\"Escaneando directorio: {self.root_dir}\")\n",
    "\n",
    "        # 1. Descubrir nombres de clases (subdirectorios)\n",
    "        subdirs = [d for d in os.listdir(self.root_dir)\n",
    "                   if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        self.class_names = sorted(subdirs)\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        if num_classes == 0:\n",
    "            raise ValueError(f\"No se encontraron subdirectorios de clases en {self.root_dir}\")\n",
    "\n",
    "        # 2. Crear mapeo class_to_label (para arrays one-hot)\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            one_hot = np.zeros(num_classes, dtype=np.float32)\n",
    "            one_hot[i] = 1.0\n",
    "            self.class_to_label[class_name] = one_hot\n",
    "\n",
    "        print(f\"Se encontraron {num_classes} clases: {self.class_names}\")\n",
    "\n",
    "        # 3. Rellenar la lista de √≠ndices maestros\n",
    "        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "        for class_index, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            one_hot_label = self.class_to_label[class_name]\n",
    "\n",
    "            # Listar archivos en el directorio de la clase\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.lower().endswith(image_extensions):\n",
    "                    filepath = os.path.join(class_path, filename)\n",
    "                    # Almacenar (filepath, one_hot_label)\n",
    "                    self.data_index.append((filepath, one_hot_label))\n",
    "                    \n",
    "                    # --- ¬°CORRECCI√ìN A√ëADIDA AQU√ç! ---\n",
    "                    # Almacenar el √≠ndice entero (0, 1, 2...)\n",
    "                    self.targets.append(class_index)\n",
    "                    # --- FIN DE LA CORRECCI√ìN ---\n",
    "\n",
    "        print(f\"Total de im√°genes indexadas: {len(self.data_index)}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Devuelve el n√∫mero total de items (im√°genes) en el dataset.\"\"\"\n",
    "        return len(self.data_index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Recupera la imagen y su etiqueta one-hot correspondiente.\n",
    "        Aplica transformaciones si se proporcionan.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.data_index) or idx < 0:\n",
    "            raise IndexError(\"√çndice fuera de rango\")\n",
    "\n",
    "        filepath, label_vector = self.data_index[idx]\n",
    "\n",
    "        # 1. Cargar la imagen con PIL\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar la imagen {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 2. Aplicar transformaciones (ej. ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Devuelve la imagen transformada y el vector one-hot\n",
    "        return image, label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc5e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el dataset completo para indexar...\n",
      "Escaneando directorio: /lustre/proyectos/p032/datasets/images/3kvasir\n",
      "Se encontraron 3 clases: ['normal-cecum', 'normal-pylorus', 'normal-z-line']\n",
      "Total de im√°genes indexadas: 1500\n",
      "Total de im√°genes encontradas: 1500\n",
      "Realizando primera divisi√≥n (estratificada)...\n",
      "Realizando segunda divisi√≥n (estratificada)...\n",
      "\n",
      "--- ¬°Divisi√≥n completada! ---\n",
      "Total:      1500\n",
      "Set Train:  1049\n",
      "Set Val:    226\n",
      "Set Test:   225\n",
      "\n",
      "DataLoaders estratificados (train, val, test) creados.\n",
      "\n",
      "Verificando distribuci√≥n (ejemplo):\n",
      "  Train: ['33.37%', '33.37%', '33.27%']\n",
      "  Val:   ['33.19%', '33.19%', '33.63%']\n",
      "  Test:  ['33.33%', '33.33%', '33.33%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Un NUEVO Dataset Wrapper (m√°s simple)\n",
    "# ---------------------------------------------------------------\n",
    "class PreSplitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Un Dataset que acepta una lista de datos (filepath, label) \n",
    "    pre-dividida en su constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list: List[Tuple[str, np.ndarray]], transform: Any = None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, np.ndarray]:\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Obtener el filepath y la etiqueta de la lista\n",
    "        filepath, label_vector = self.data_list[idx]\n",
    "\n",
    "        # Cargar la imagen\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            raise\n",
    "            \n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label_vector\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Configuraci√≥n y Proceso de Divisi√≥n\n",
    "# ---------------------------------------------------------------\n",
    "# --- Configuraci√≥n ---\n",
    "dataset_root = \"/lustre/proyectos/p032/datasets/images/3kvasir\"\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# Definir los ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15 # (debe sumar 1.0)\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 1. Cargar el dataset COMPLETO ---\n",
    "print(\"Cargando el dataset completo para indexar...\")\n",
    "# (Necesitamos la clase 'ImageDatasetWrapper' original para esto)\n",
    "# (He a√±adido .targets a la clase para que esto funcione)\n",
    "full_dataset = ImageDatasetWrapper(root_dir=dataset_root)\n",
    "\n",
    "# Extraer los datos y las etiquetas para sklearn\n",
    "# data_index es List[Tuple[str, np.ndarray]]\n",
    "# targets es List[int] (ej. 0, 1, 2, 0, 1...)\n",
    "all_data = full_dataset.data_index \n",
    "all_targets = full_dataset.targets \n",
    "\n",
    "if len(all_data) == 0:\n",
    "    raise RuntimeError(\"Error: No se encontraron datos en el dataset.\")\n",
    "\n",
    "print(f\"Total de im√°genes encontradas: {len(all_data)}\")\n",
    "\n",
    "# --- 2. Primera Divisi√≥n (Train+Val vs Test) ---\n",
    "# Dividimos el 85% para (train+val) y el 15% para test\n",
    "print(\"Realizando primera divisi√≥n (estratificada)...\")\n",
    "train_val_data, test_data, train_val_targets, test_targets = train_test_split(\n",
    "    all_data,\n",
    "    all_targets,\n",
    "    test_size=TEST_RATIO,\n",
    "    stratify=all_targets, # ¬°La clave es esta!\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# --- 3. Segunda Divisi√≥n (Train vs Val) ---\n",
    "# Dividimos (train+val) en train y val\n",
    "# El ratio debe recalcularse: VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "val_split_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "\n",
    "print(\"Realizando segunda divisi√≥n (estratificada)...\")\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(\n",
    "    train_val_data,\n",
    "    train_val_targets,\n",
    "    test_size=val_split_ratio,\n",
    "    stratify=train_val_targets, # Estratificar de nuevo\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n--- ¬°Divisi√≥n completada! ---\")\n",
    "print(f\"Total:      {len(all_data)}\")\n",
    "print(f\"Set Train:  {len(train_data)}\")\n",
    "print(f\"Set Val:    {len(val_data)}\")\n",
    "print(f\"Set Test:   {len(test_data)}\")\n",
    "\n",
    "# --- 4. Crear los Datasets y DataLoaders ---\n",
    "\n",
    "# Aplicar la transformaci√≥n a cada set\n",
    "train_dataset = PreSplitDataset(train_data, transform=transform)\n",
    "val_dataset = PreSplitDataset(val_data, transform=transform)\n",
    "test_dataset = PreSplitDataset(test_data, transform=transform)\n",
    "\n",
    "# Crear los DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"\\nDataLoaders estratificados (train, val, test) creados.\")\n",
    "\n",
    "# --- 5. (Opcional) Verificar la distribuci√≥n de clases ---\n",
    "print(\"\\nVerificando distribuci√≥n (ejemplo):\")\n",
    "\n",
    "def get_class_counts(targets_list):\n",
    "    counts = np.bincount(targets_list)\n",
    "    return [f\"{count/len(targets_list)*100:.2f}%\" for count in counts]\n",
    "    \n",
    "print(f\"  Train: {get_class_counts(train_targets)}\")\n",
    "print(f\"  Val:   {get_class_counts(val_targets)}\")\n",
    "print(f\"  Test:  {get_class_counts(test_targets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b9f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "N√∫mero de clases: 3\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURACI√ìN INICIAL ---\n",
    "# ==========================================================\n",
    "# PATH_MODELO_SSL = \"/lustre/proyectos/p032/models/multi_pretext_model2.ckpt\" # No se usa\n",
    "# MODEL_PATH = \"/lustre/home/opacheco/MEDA_Challenge/models/221025MG_backbone.ssl.pth\" # No se usa\n",
    "\n",
    "# ¬øCu√°ntas clases tiene tu dataset de PRUEBA?\n",
    "NUM_CLASES = 3 # Esto sigue siendo correcto para tu 3kvasir\n",
    "\n",
    "# Par√°metros (¬°Importante usar los mismos!)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_DE_PRUEBA = 10\n",
    "LEARNING_RATE = 0.001 # Este LR se us√≥ para el Linear Probe, ¬°mantenerlo!\n",
    "# JIGSAW_N = 2 # No aplica aqu√≠\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "print(f\"N√∫mero de clases: {NUM_CLASES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caae3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando ResNet-18 pre-entrenado en ImageNet...\n",
      "¬°Backbone ResNet-18 (ImageNet) cargado!\n"
     ]
    }
   ],
   "source": [
    "# --- Cargar el Backbone Baseline (ResNet-18 ImageNet) ---\n",
    "\n",
    "print(\"Cargando ResNet-18 pre-entrenado en ImageNet...\")\n",
    "try:\n",
    "    # Volvemos a resnet18\n",
    "    resnet_imagenet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "except AttributeError:\n",
    "    print(\"...usando fallback 'pretrained=True' por versi√≥n de torchvision.\")\n",
    "    resnet_imagenet = models.resnet18(pretrained=True)\n",
    "\n",
    "baseline_backbone = nn.Sequential(*list(resnet_imagenet.children())[:-1])\n",
    "print(\"¬°Backbone ResNet-18 (ImageNet) cargado!\")\n",
    "\n",
    "# Mover a GPU\n",
    "baseline_backbone = baseline_backbone.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98995d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando CrossEntropyLoss para multi-clase.\n",
      "Modelo Linear Probing (ResNet-18 Baseline) creado CORRECTAMENTE.\n"
     ]
    }
   ],
   "source": [
    "# --- Crear el Modelo para Linear Probing (ResNet-18 Baseline) ---\n",
    "\n",
    "# Congelar todo el backbone baseline (el ResNet-18 de ImageNet)\n",
    "for param in baseline_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- ¬°CAMBIO IMPORTANTE! ---\n",
    "# La salida de ResNet-18 es 512\n",
    "in_features = 512\n",
    "# Crear la cabeza lineal CORRECTA\n",
    "linear_head = nn.Linear(in_features, NUM_CLASES) # <-- Debe ser 512\n",
    "\n",
    "# Clase para el modelo combinado (Backbone + Cabeza)\n",
    "class LinearProbingModel(nn.Module):\n",
    "    def __init__(self, backbone, linear_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.linear_head = linear_head # <-- Ahora s√≠ recibe la cabeza correcta (512 -> 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Asegurar que el backbone est√© en modo eval\n",
    "        self.backbone.eval()\n",
    "        with torch.no_grad(): # No calcular gradientes para el backbone\n",
    "            feats = self.backbone(x)          # [B, 512, 1, 1]\n",
    "\n",
    "        feats = feats.view(feats.size(0), -1)  # Flatten -> [B, 512]\n",
    "        out = self.linear_head(feats)        # [B, NUM_CLASES] - ¬°Ahora s√≠ funciona!\n",
    "        return out\n",
    "\n",
    "# Crear la instancia del modelo final\n",
    "# (baseline_backbone debe ser tu ResNet-18 cargado en la celda anterior)\n",
    "model = LinearProbingModel(baseline_backbone, linear_head).to(DEVICE)\n",
    "\n",
    "# Configurar Loss y Optimizador\n",
    "# Para 3kvasir (NUM_CLASES=3), CrossEntropyLoss es correcto\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Usando CrossEntropyLoss para multi-clase.\")\n",
    "\n",
    "# Optimizador SOLO para la cabeza lineal, usando el LEARNING_RATE definido (0.001)\n",
    "optimizer = optim.Adam(model.linear_head.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Modelo Linear Probing (ResNet-18 Baseline) creado CORRECTAMENTE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7095946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\n",
      "Epoch 1/10 - Train Loss: 0.0742 - Val Loss: 0.0716 - Val Acc: 99.12%\n",
      "Epoch 2/10 - Train Loss: 0.0715 - Val Loss: 0.0721 - Val Acc: 98.23%\n",
      "Epoch 3/10 - Train Loss: 0.0680 - Val Loss: 0.0700 - Val Acc: 98.67%\n",
      "Epoch 4/10 - Train Loss: 0.0649 - Val Loss: 0.0671 - Val Acc: 99.12%\n",
      "Epoch 5/10 - Train Loss: 0.0620 - Val Loss: 0.0651 - Val Acc: 98.67%\n",
      "Epoch 6/10 - Train Loss: 0.0609 - Val Loss: 0.0637 - Val Acc: 98.67%\n",
      "Epoch 7/10 - Train Loss: 0.0582 - Val Loss: 0.0644 - Val Acc: 97.79%\n",
      "Epoch 8/10 - Train Loss: 0.0570 - Val Loss: 0.0641 - Val Acc: 97.79%\n",
      "Epoch 9/10 - Train Loss: 0.0544 - Val Loss: 0.0613 - Val Acc: 98.67%\n",
      "Epoch 10/10 - Train Loss: 0.0530 - Val Loss: 0.0600 - Val Acc: 98.67%\n",
      "Entrenamiento de la cabeza finalizado.\n"
     ]
    }
   ],
   "source": [
    "# --- ENTRENAR LA CABEZA LINEAL CON VALIDACI√ìN ---\n",
    "\n",
    "print(\"Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\")\n",
    "\n",
    "for epoch in range(EPOCHS_DE_PRUEBA):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # ---- Train ----\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validaci√≥n ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_one_hot in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels_one_hot = labels_one_hot.to(DEVICE)\n",
    "\n",
    "            labels_indices = torch.argmax(labels_one_hot, dim=1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels_indices)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels_indices.size(0)\n",
    "            correct += (predicted == labels_indices).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_DE_PRUEBA} - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Entrenamiento de la cabeza finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e909a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en el set de validaci√≥n...\n",
      "\\n==========================================================\n",
      "üéâ ¬°Prueba de Evaluaci√≥n Lineal (Linear Probing - ResNet50 Baseline) completa! üéâ\n",
      "   Accuracy en el set de test: 97.78 %\n",
      "   F1 Score (Macro) en el set de test: 97.78 %\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- 6. EVALUAR EL RENDIMIENTO CON F1 SCORE ---\n",
    "\n",
    "print(\"Evaluando en el set de validaci√≥n...\")\n",
    "\n",
    "# Lista para almacenar todas las etiquetas verdaderas y predichas\n",
    "all_labels = []\n",
    "all_predicted = []\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_one_hot in test_loader:\n",
    "        # Mover datos al dispositivo (CPU/GPU)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels_one_hot = labels_one_hot.to(DEVICE)\n",
    "\n",
    "        labels_indices = torch.argmax(labels_one_hot, dim=1)\n",
    "        \n",
    "        # 1. Pase adelante (Forward Pass)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2. Obtener la predicci√≥n de clase\n",
    "        _, predicted_indices = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 3. Almacenar para el c√°lculo de F1 Score\n",
    "        # Mover a CPU para Scikit-learn y convertir a numpy\n",
    "        all_labels.extend(labels_indices.cpu().numpy())\n",
    "        all_predicted.extend(predicted_indices.cpu().numpy())\n",
    "        \n",
    "        # 4. Actualizar contadores de Accuracy\n",
    "        total += labels_one_hot.size(0)\n",
    "        correct += (predicted_indices == labels_indices).sum().item()\n",
    "\n",
    "\n",
    "# --- C√ÅLCULO DE M√âTRICAS ---\n",
    "\n",
    "# 1. Calcular Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# 2. Calcular F1 Score\n",
    "# 'average=\"macro\"' se usa com√∫nmente en problemas multi-clase para dar\n",
    "# igual peso a cada clase, independientemente del desequilibrio.\n",
    "# Cambiar a 'average=\"weighted\"' si se necesita considerar el desequilibrio de clases.\n",
    "f1 = f1_score(all_labels, all_predicted, average='macro') \n",
    "f1_percentage = f1 * 100\n",
    "\n",
    "# --- RESULTADO FINAL ---\n",
    "print(\"\\\\n==========================================================\") \n",
    "print(f\"üéâ ¬°Prueba de Evaluaci√≥n Lineal (Linear Probing - ResNet50 Baseline) completa! üéâ\")\n",
    "print(f\"   Accuracy en el set de test: {accuracy:.2f} %\") \n",
    "print(f\"   F1 Score (Macro) en el set de test: {f1_percentage:.2f} %\") \n",
    "print(\"==========================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
