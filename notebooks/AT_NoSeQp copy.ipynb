{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f969aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import RichProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd90ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0. Configuraci√≥n ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = \"/lustre/home/atorres/compartido/datasets/all_medmnist_images\" # <- Tu carpeta con todas las imgs\n",
    "MODEL_PATH = \"/lustre/home/atorres/MEDA_Challenge/models/221025MG_backbone.ssl.pth\" # <- Tu modelo\n",
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 10\n",
    "LR = 1e-4 # Learning rate para el fine-tuning\n",
    "JIGSAW_N = 4 # Rejilla de 4x4 para Jigsaw\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc73a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando backbone SSL...\n",
      "Backbone cargado y movido a GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Cargar el Backbone SSL Pre-entrenado ---\n",
    "\n",
    "# Definimos una clase 'dummy' solo para cargar la estructura que guardaste\n",
    "# (Asumiendo que guardaste solo el state_dict de 'encoder_q[0]')\n",
    "class MoCoLightning(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder_q = nn.Sequential(backbone)\n",
    "\n",
    "print(\"Cargando backbone SSL...\")\n",
    "# Cargar ResNet18 sin pesos (solo la arquitectura)\n",
    "resnet = models.resnet18(weights=None) \n",
    "# Tu backbone (quitando la capa FC final)\n",
    "backbone_structure = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Cargar el estado\n",
    "encoder_wrapper = MoCoLightning(backbone=backbone_structure)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Cargar los pesos en la estructura\n",
    "encoder_wrapper.encoder_q[0].load_state_dict(state_dict)\n",
    "\n",
    "# --- Este es tu backbone listo para usar ---\n",
    "ssl_backbone = encoder_wrapper.encoder_q[0].to(DEVICE)\n",
    "print(\"Backbone cargado y movido a GPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71417419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Dataset (Versi√≥n Carpeta Unificada) ---\n",
    "\n",
    "class MedMNISTUnifiedFolder(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset que lee todas las im√°genes de una sola carpeta ra√≠z.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.files = [os.path.join(root, f) for f in os.listdir(root)\n",
    "                      if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23826e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado con 600338 im√°genes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformaci√≥n base (para el dataloader)\n",
    "transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    # Nota: No normalizamos aqu√≠ porque las tareas (color, patch)\n",
    "    # esperan la imagen en rango [0, 1]\n",
    "])\n",
    "\n",
    "dataset = MedMNISTUnifiedFolder(DATA_PATH, transform)\n",
    "loader = DataLoader(dataset, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True, \n",
    "                    num_workers=2, \n",
    "                    pin_memory=True,\n",
    "                    drop_last=True) # drop_last=True es importante para Jigsaw si el batch no es divisible\n",
    "print(f\"Dataset cargado con {len(dataset)} im√°genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01decae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 3. Funciones de Pretext-Task (VERSI√ìN TENSOR) ---\n",
    "# Estas funciones operan sobre BATCHES en la GPU\n",
    "\n",
    "def colorization_pair_tensor(imgs):\n",
    "    \"\"\"\n",
    "    Input: Batch de imgs RGB [B, 3, H, W]\n",
    "    Output: (Input para modelo [B, 3, H, W] (gris), Target [B, 3, H, W] (color))\n",
    "    \"\"\"\n",
    "    # 1. Convertir a escala de grises (usando la transformaci√≥n de torchvision)\n",
    "    gray = T.Grayscale()(imgs) # Shape: [B, 1, H, W]\n",
    "    \n",
    "    # 2. REPETIR el canal 1 -> 3 (Esta es la correcci√≥n para el RuntimeError)\n",
    "    gray_repeated = gray.repeat(1, 3, 1, 1) # Shape: [B, 3, H, W]\n",
    "    \n",
    "    # Input: gris repetido, Target: imagen original a color\n",
    "    return gray_repeated, imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79916b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_prediction_pair_tensor(imgs, mask_size_ratio=0.25):\n",
    "    \"\"\"\n",
    "    Input: Batch de imgs RGB [B, 3, H, W]\n",
    "    Output: (Input para modelo [B, 3, H, W] (con m√°scara), Target [B, 3, H, W] (original))\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    mask_size_h = int(H * mask_size_ratio)\n",
    "    mask_size_w = int(W * mask_size_ratio)\n",
    "    \n",
    "    # Calcular centro\n",
    "    x = (W - mask_size_w) // 2\n",
    "    y = (H - mask_size_h) // 2\n",
    "    \n",
    "    masked_imgs = imgs.clone()\n",
    "    # Enmascarar con 0.0 (negro)\n",
    "    masked_imgs[:, :, y:y+mask_size_h, x:x+mask_size_w] = 0.0 \n",
    "    \n",
    "    return masked_imgs, imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ec2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jigsaw_pair_tensor(imgs, n=JIGSAW_N):\n",
    "    \"\"\"\n",
    "    Input: Batch de imgs RGB [B, 3, H, W]\n",
    "    Output: (Input para modelo [B, 3, H, W] (desordenado), Target [B, n*n] (orden))\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    patch_h, patch_w = H // n, W // n\n",
    "    \n",
    "    if H % n != 0 or W % n != 0:\n",
    "        # Esto no deber√≠a pasar si IMG_SIZE es 128 y n=3. Ajusta si es necesario.\n",
    "        raise ValueError(f\"El tama√±o de la imagen ({H}x{W}) no es divisible por n={n}\")\n",
    "\n",
    "    # 1. Cortar el batch en parches\n",
    "    # imgs shape: [B, C, H, W] -> [B, C, n, patch_h, n, patch_w]\n",
    "    patches = imgs.unfold(2, patch_h, patch_h).unfold(3, patch_w, patch_w)\n",
    "    # -> [B, C, n, n, patch_h, patch_w]\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B, n*n, C, patch_h, patch_w)\n",
    "    # -> [B, n*n, C, patch_h, patch_w] (Batch, NumPatches, C, H_patch, W_patch)\n",
    "\n",
    "    # 2. Generar permutaci√≥n aleatoria para cada imagen en el batch\n",
    "    # 'order' es el target: [B, n*n]. \n",
    "    # Cada fila es [0, 1, ..., 8] desordenado.\n",
    "    order = torch.stack([torch.randperm(n*n, device=imgs.device) for _ in range(B)])\n",
    "    \n",
    "    # 3. Desordenar los parches usando el 'order'\n",
    "    # 'order' [B, 9] -> expand a [B, 9, C, pH, pW] para gather\n",
    "    order_expanded = order.view(B, n*n, 1, 1, 1).expand_as(patches)\n",
    "    shuffled_patches = torch.gather(patches, 1, order_expanded)\n",
    "    # -> [B, n*n, C, patch_h, patch_w]\n",
    "\n",
    "    # 4. Reensamblar el batch desordenado\n",
    "    shuffled_patches = shuffled_patches.view(B, n, n, C, patch_h, patch_w)\n",
    "    shuffled_patches = shuffled_patches.permute(0, 3, 1, 4, 2, 5) \n",
    "    # -> [B, C, n, patch_h, n, patch_w]\n",
    "    shuffled_imgs = shuffled_patches.reshape(B, C, H, W)\n",
    "    \n",
    "    # Input: imagen desordenada, Target: el orden (permutaci√≥n)\n",
    "    return shuffled_imgs, order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08473931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Modelo Multi-Pretexto (Versi√≥n LightningModule) ---\n",
    "\n",
    "class MultiPretextSSL_Lightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate') # Guarda lr\n",
    "        self.backbone = backbone\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        num_features = 512 # Salida de ResNet18\n",
    "        \n",
    "        # --- DECODER CORREGIDO PARA 28x28 ---\n",
    "        decoder_layers_28x28 = [\n",
    "            nn.ConvTranspose2d(num_features, 256, kernel_size=4, stride=1, padding=0), # 1x1 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1), # 4x4 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),    # 14x14 -> 28x28\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        self.color_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        self.patch_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        \n",
    "        # --- JIGSAW HEAD CORREGIDO PARA N=4 (16 patches) ---\n",
    "        self.n_patches = JIGSAW_N * JIGSAW_N # 16\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_patches * self.n_patches) # 16*16 = 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x, task=\"color\"):\n",
    "        feats = self.backbone(x)\n",
    "        if task == \"color\":\n",
    "            return self.color_head(feats)\n",
    "        elif task == \"patch\":\n",
    "            return self.patch_head(feats)\n",
    "        elif task == \"jigsaw\":\n",
    "            return self.jigsaw_head(feats)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch es lo que retorna __getitem__, en este caso, 'imgs'\n",
    "        imgs = batch\n",
    "        \n",
    "        # Elegir una tarea al azar\n",
    "        task = random.choice([\"color\", \"patch\", \"jigsaw\"])\n",
    "        loss = 0.0\n",
    "\n",
    "        if task == \"color\":\n",
    "            inp, target = colorization_pair_tensor(imgs)\n",
    "            pred = self(inp, \"color\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"patch\":\n",
    "            inp, target = patch_prediction_pair_tensor(imgs)\n",
    "            pred = self(inp, \"patch\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"jigsaw\":\n",
    "            inp, target = jigsaw_pair_tensor(imgs, n=JIGSAW_N) # n=4\n",
    "            pred = self(inp, \"jigsaw\") # Shape: [B, 256]\n",
    "            \n",
    "            # [B, 256] -> [B, 16, 16]\n",
    "            pred_reshaped = pred.view(-1, self.n_patches, self.n_patches) \n",
    "            target_reshaped = target.view(-1) # [B*16]\n",
    "            \n",
    "            loss = F.cross_entropy(pred_reshaped.view(-1, self.n_patches), target_reshaped)\n",
    "\n",
    "        # Loggear la p√©rdida. 'prog_bar=True' la muestra en la barra de progreso\n",
    "        self.log(f'loss_{task}', loss, prog_bar=True)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c128f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando entrenamiento con Lightning ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mN_EPOCHS,\n\u001b[1;32m     11\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Usa 'gpu' (Lightning 2.0+)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Puedes a√±adir un logger si quieres (ej. TensorBoardLogger)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ¬°A entrenar!\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Aqu√≠ es donde ver√°s la barra de progreso con el ETA (tiempo restante)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Entrenamiento finalizado ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:560\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:110\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches processes that run the given function in parallel.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mThe function is allowed to have a return value. However, when all processes join, only the return value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 110\u001b[0m     \u001b[43m_check_bad_cuda_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     _check_missing_main_guard()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/strategies/launchers/multiprocessing.py:208\u001b[0m, in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE:\n\u001b[1;32m    207\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You will have to restart the Python kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
     ]
    }
   ],
   "source": [
    "# --- 5. Bucle de Entrenamiento (¬°Ahora con el Trainer!) ---\n",
    "\n",
    "print(\"--- Iniciando entrenamiento con Lightning ---\")\n",
    "\n",
    "# Instanciar el modelo de Lightning\n",
    "model = MultiPretextSSL_Lightning(ssl_backbone, learning_rate=LR)\n",
    "\n",
    "# Instanciar el Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator='gpu',  # Usa 'gpu' (Lightning 2.0+)\n",
    "    devices=-1,          # Usa 1 GPU\n",
    "    callbacks=[RichProgressBar()], # Una barra de progreso m√°s bonita\n",
    "    logger=None # Puedes a√±adir un logger si quieres (ej. TensorBoardLogger)\n",
    ")\n",
    "\n",
    "# ¬°A entrenar!\n",
    "# Aqu√≠ es donde ver√°s la barra de progreso con el ETA (tiempo restante)\n",
    "trainer.fit(model, loader)\n",
    "\n",
    "print(\"--- Entrenamiento finalizado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de295fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # --- 6. Exportaci√≥n a ONNX ---\n",
    "# # Exportamos solo el backbone adaptado, que es lo que usar√°s \n",
    "# # para la inferencia downstream (clustering, clasificaci√≥n, etc.)\n",
    "\n",
    "# print(\"Exportando backbone adaptado a ONNX...\")\n",
    "# model.eval()\n",
    "# dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
    "\n",
    "# # Exportamos solo 'model.backbone'\n",
    "# torch.onnx.export(\n",
    "#     model.backbone, \n",
    "#     dummy_input, \n",
    "#     \"ssl_adapted_backbone.onnx\", # Nombre del archivo\n",
    "#     input_names=['input'], \n",
    "#     output_names=['features'],\n",
    "#     opset_version=17,\n",
    "#     dynamic_axes={'input': {0: 'batch_size'}, 'features': {0: 'batch_size'}}\n",
    "# )\n",
    "                      \n",
    "# print(\"Backbone adaptado guardado en 'ssl_adapted_backbone.onnx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
