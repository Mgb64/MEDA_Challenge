{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f969aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import RichProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd90ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0. ConfiguraciÃ³n ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = \"/lustre/home/atorres/compartido/datasets/all_medmnist_images\" # <- Tu carpeta con todas las imgs\n",
    "MODEL_PATH = \"/lustre/home/atorres/MEDA_Challenge/models/221025MG_backbone.ssl.pth\" # <- Tu modelo\n",
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4 # Learning rate para el fine-tuning\n",
    "JIGSAW_N = 2 # Rejilla de 4x4 para Jigsaw\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc73a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando backbone SSL...\n",
      "Backbone cargado y movido a GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Cargar el Backbone SSL Pre-entrenado ---\n",
    "\n",
    "# Definimos una clase 'dummy' solo para cargar la estructura que guardaste\n",
    "# (Asumiendo que guardaste solo el state_dict de 'encoder_q[0]')\n",
    "class MoCoLightning(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder_q = nn.Sequential(backbone)\n",
    "\n",
    "print(\"Cargando backbone SSL...\")\n",
    "# Cargar ResNet18 sin pesos (solo la arquitectura)\n",
    "resnet = models.resnet18(weights=None) \n",
    "# Tu backbone (quitando la capa FC final)\n",
    "backbone_structure = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Cargar el estado\n",
    "encoder_wrapper = MoCoLightning(backbone=backbone_structure)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Cargar los pesos en la estructura\n",
    "encoder_wrapper.encoder_q[0].load_state_dict(state_dict)\n",
    "\n",
    "# --- Este es tu backbone listo para usar ---\n",
    "ssl_backbone = encoder_wrapper.encoder_q[0].to(DEVICE)\n",
    "print(\"Backbone cargado y movido a GPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71417419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Dataset (VersiÃ³n Carpeta Unificada) ---\n",
    "\n",
    "class MedMNISTUnifiedFolder(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset que lee todas las imÃ¡genes de una sola carpeta raÃ­z.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.files = [os.path.join(root, f) for f in os.listdir(root)\n",
    "                      if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23826e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado con 600338 imÃ¡genes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TransformaciÃ³n base (para el dataloader)\n",
    "transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    # Nota: No normalizamos aquÃ­ porque las tareas (color, patch)\n",
    "    # esperan la imagen en rango [0, 1]\n",
    "])\n",
    "\n",
    "dataset = MedMNISTUnifiedFolder(DATA_PATH, transform)\n",
    "loader = DataLoader(dataset, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True, \n",
    "                    num_workers=2, \n",
    "                    pin_memory=True,\n",
    "                    drop_last=True) # drop_last=True es importante para Jigsaw si el batch no es divisible\n",
    "print(f\"Dataset cargado con {len(dataset)} imÃ¡genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01decae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 3. Funciones de Pretext-Task (VERSIÃ“N TENSOR) ---\n",
    "# Estas funciones operan sobre BATCHES en la GPU\n",
    "\n",
    "def colorization_pair_tensor(imgs):\n",
    "    \"\"\"\n",
    "    Input: Batch de imgs RGB [B, 3, H, W]\n",
    "    Output: (Input para modelo [B, 3, H, W] (gris), Target [B, 3, H, W] (color))\n",
    "    \"\"\"\n",
    "    # 1. Convertir a escala de grises (usando la transformaciÃ³n de torchvision)\n",
    "    gray = T.Grayscale()(imgs) # Shape: [B, 1, H, W]\n",
    "    \n",
    "    # 2. REPETIR el canal 1 -> 3 (Esta es la correcciÃ³n para el RuntimeError)\n",
    "    gray_repeated = gray.repeat(1, 3, 1, 1) # Shape: [B, 3, H, W]\n",
    "    \n",
    "    # Input: gris repetido, Target: imagen original a color\n",
    "    return gray_repeated, imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79916b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_prediction_pair_tensor(imgs, mask_size_ratio=0.25):\n",
    "    \"\"\"\n",
    "    Input: Batch de imgs RGB [B, 3, H, W]\n",
    "    Output: (Input para modelo [B, 3, H, W] (con mÃ¡scara), Target [B, 3, H, W] (original))\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    mask_size_h = int(H * mask_size_ratio)\n",
    "    mask_size_w = int(W * mask_size_ratio)\n",
    "    \n",
    "    # Calcular centro\n",
    "    x = (W - mask_size_w) // 2\n",
    "    y = (H - mask_size_h) // 2\n",
    "    \n",
    "    masked_imgs = imgs.clone()\n",
    "    # Enmascarar con 0.0 (negro)\n",
    "    masked_imgs[:, :, y:y+mask_size_h, x:x+mask_size_w] = 0.0 \n",
    "    \n",
    "    return masked_imgs, imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ec2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jigsaw_pair_tensor(imgs, n=JIGSAW_N):\n",
    "    \"\"\"\n",
    "    Input: Batch de imgs RGB [B, 3, H, W]\n",
    "    Output: (Input para modelo [B, 3, H, W] (desordenado), Target [B, n*n] (orden))\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    patch_h, patch_w = H // n, W // n\n",
    "    \n",
    "    if H % n != 0 or W % n != 0:\n",
    "        # Esto no deberÃ­a pasar si IMG_SIZE es 128 y n=3. Ajusta si es necesario.\n",
    "        raise ValueError(f\"El tamaÃ±o de la imagen ({H}x{W}) no es divisible por n={n}\")\n",
    "\n",
    "    # 1. Cortar el batch en parches\n",
    "    # imgs shape: [B, C, H, W] -> [B, C, n, patch_h, n, patch_w]\n",
    "    patches = imgs.unfold(2, patch_h, patch_h).unfold(3, patch_w, patch_w)\n",
    "    # -> [B, C, n, n, patch_h, patch_w]\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B, n*n, C, patch_h, patch_w)\n",
    "    # -> [B, n*n, C, patch_h, patch_w] (Batch, NumPatches, C, H_patch, W_patch)\n",
    "\n",
    "    # 2. Generar permutaciÃ³n aleatoria para cada imagen en el batch\n",
    "    # 'order' es el target: [B, n*n]. \n",
    "    # Cada fila es [0, 1, ..., 8] desordenado.\n",
    "    order = torch.stack([torch.randperm(n*n, device=imgs.device) for _ in range(B)])\n",
    "    \n",
    "    # 3. Desordenar los parches usando el 'order'\n",
    "    # 'order' [B, 9] -> expand a [B, 9, C, pH, pW] para gather\n",
    "    order_expanded = order.view(B, n*n, 1, 1, 1).expand_as(patches)\n",
    "    shuffled_patches = torch.gather(patches, 1, order_expanded)\n",
    "    # -> [B, n*n, C, patch_h, patch_w]\n",
    "\n",
    "    # 4. Reensamblar el batch desordenado\n",
    "    shuffled_patches = shuffled_patches.view(B, n, n, C, patch_h, patch_w)\n",
    "    shuffled_patches = shuffled_patches.permute(0, 3, 1, 4, 2, 5) \n",
    "    # -> [B, C, n, patch_h, n, patch_w]\n",
    "    shuffled_imgs = shuffled_patches.reshape(B, C, H, W)\n",
    "    \n",
    "    # Input: imagen desordenada, Target: el orden (permutaciÃ³n)\n",
    "    return shuffled_imgs, order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08473931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Modelo Multi-Pretexto (VersiÃ³n LightningModule) ---\n",
    "\n",
    "class MultiPretextSSL_Lightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate') # Guarda lr\n",
    "        self.backbone = backbone\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        num_features = 512 # Salida de ResNet18\n",
    "        \n",
    "        # --- DECODER CORREGIDO PARA 28x28 ---\n",
    "        decoder_layers_28x28 = [\n",
    "            nn.ConvTranspose2d(num_features, 256, kernel_size=4, stride=1, padding=0), # 1x1 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1), # 4x4 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),    # 14x14 -> 28x28\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        self.color_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        self.patch_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        \n",
    "        # --- JIGSAW HEAD CORREGIDO PARA N=4 (16 patches) ---\n",
    "        self.n_patches = JIGSAW_N * JIGSAW_N # 16\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_patches * self.n_patches) # 16*16 = 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x, task=\"color\"):\n",
    "        feats = self.backbone(x)\n",
    "        if task == \"color\":\n",
    "            return self.color_head(feats)\n",
    "        elif task == \"patch\":\n",
    "            return self.patch_head(feats)\n",
    "        elif task == \"jigsaw\":\n",
    "            return self.jigsaw_head(feats)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch es lo que retorna __getitem__, en este caso, 'imgs'\n",
    "        imgs = batch\n",
    "        \n",
    "        # Elegir una tarea al azar\n",
    "        task = random.choice([\"color\", \"patch\", \"jigsaw\"])\n",
    "        loss = 0.0\n",
    "\n",
    "        if task == \"color\":\n",
    "            inp, target = colorization_pair_tensor(imgs)\n",
    "            pred = self(inp, \"color\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"patch\":\n",
    "            inp, target = patch_prediction_pair_tensor(imgs)\n",
    "            pred = self(inp, \"patch\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"jigsaw\":\n",
    "            inp, target = jigsaw_pair_tensor(imgs, n=JIGSAW_N) # n=4\n",
    "            pred = self(inp, \"jigsaw\") # Shape: [B, 256]\n",
    "            \n",
    "            # [B, 256] -> [B, 16, 16]\n",
    "            pred_reshaped = pred.view(-1, self.n_patches, self.n_patches) \n",
    "            target_reshaped = target.view(-1) # [B*16]\n",
    "            \n",
    "            loss = F.cross_entropy(pred_reshaped.view(-1, self.n_patches), target_reshaped)\n",
    "\n",
    "        # Loggear la pÃ©rdida. 'prog_bar=True' la muestra en la barra de progreso\n",
    "        self.log(f'loss_{task}', loss, prog_bar=True)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c128f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /lustre/proyectos/p032/env/lib64/python3.9/site-pack ...\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/lustre/proyectos/p032/env/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('AMD Instinct MI210') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando entrenamiento con Lightning ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name        </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ backbone    â”‚ Sequential â”‚ 11.2 M â”‚ train â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ color_head  â”‚ Sequential â”‚  2.5 M â”‚ train â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>â”‚ patch_head  â”‚ Sequential â”‚  2.5 M â”‚ train â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>â”‚ jigsaw_head â”‚ Sequential â”‚  270 K â”‚ train â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName       \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ backbone    â”‚ Sequential â”‚ 11.2 M â”‚ train â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ color_head  â”‚ Sequential â”‚  2.5 M â”‚ train â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0mâ”‚ patch_head  â”‚ Sequential â”‚  2.5 M â”‚ train â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0mâ”‚ jigsaw_head â”‚ Sequential â”‚  270 K â”‚ train â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 14.0 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 14.0 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 55                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 82                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 14.0 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 14.0 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 55                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 82                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31686341c6fc4453bc076385c7adc4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 5. Bucle de Entrenamiento (Â¡Ahora con el Trainer!) ---\n",
    "\n",
    "print(\"--- Iniciando entrenamiento con Lightning ---\")\n",
    "\n",
    "# Instanciar el modelo de Lightning\n",
    "model = MultiPretextSSL_Lightning(ssl_backbone, learning_rate=LR)\n",
    "\n",
    "# Instanciar el Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator='gpu',  # Usa 'gpu' (Lightning 2.0+)\n",
    "    devices=1,          # Usa 1 GPU\n",
    "    callbacks=[RichProgressBar()], # Una barra de progreso mÃ¡s bonita\n",
    "    logger=None # Puedes aÃ±adir un logger si quieres (ej. TensorBoardLogger)\n",
    ")\n",
    "\n",
    "# Â¡A entrenar!\n",
    "# AquÃ­ es donde verÃ¡s la barra de progreso con el ETA (tiempo restante)\n",
    "trainer.fit(model, loader)\n",
    "\n",
    "print(\"--- Entrenamiento finalizado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd9a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint del modelo Lightning guardado en 'multi_pretext_model.ckpt'\n"
     ]
    }
   ],
   "source": [
    "trainer.save_checkpoint(\"/lustre/home/atorres/MEDA_Challenge/models/multi_pretext_model2.ckpt\")\n",
    "print(\"Checkpoint del modelo Lightning guardado en 'multi_pretext_model2.ckpt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de295fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # --- 6. ExportaciÃ³n a ONNX ---\n",
    "# # Exportamos solo el backbone adaptado, que es lo que usarÃ¡s \n",
    "# # para la inferencia downstream (clustering, clasificaciÃ³n, etc.)\n",
    "\n",
    "# print(\"Exportando backbone adaptado a ONNX...\")\n",
    "# model.eval()\n",
    "# dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
    "\n",
    "# # Exportamos solo 'model.backbone'\n",
    "# torch.onnx.export(\n",
    "#     model.backbone, \n",
    "#     dummy_input, \n",
    "#     \"ssl_adapted_backbone.onnx\", # Nombre del archivo\n",
    "#     input_names=['input'], \n",
    "#     output_names=['features'],\n",
    "#     opset_version=17,\n",
    "#     dynamic_axes={'input': {0: 'batch_size'}, 'features': {0: 'batch_size'}}\n",
    "# )\n",
    "                      \n",
    "# print(\"Backbone adaptado guardado en 'ssl_adapted_backbone.onnx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
