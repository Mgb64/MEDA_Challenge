{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde76258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Backbone cargado y movido a GPU.\n",
      "Dataset completo cargado desde: /lustre/proyectos/p032/datasets/images/3kvasir\n",
      "Total de im谩genes encontradas: 1500\n",
      "Clases (carpetas) encontradas: ['normal-cecum', 'normal-pylorus', 'normal-z-line']\n",
      "Dividiendo dataset (estratificado): 1050 (Train), 149 (Val), 301 (Test)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearProbingModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# --- FIN DEL CAMBIO DE DIVISIN ---\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Crear la cabeza lineal AHORA, asegur谩ndonos de que NUM_CLASES es correcto\u001b[39;00m\n\u001b[1;32m    201\u001b[0m linear_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features, NUM_CLASES)\n\u001b[0;32m--> 202\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLinearProbingModel\u001b[49m(model\u001b[38;5;241m.\u001b[39mbackbone, linear_head)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    205\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    206\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearProbingModel' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "# CAMBIADO: random_split ya no se usa, importamos Subset\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "# AADIDO: ImageFolder es la herramienta est谩ndar para tu estructura de dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "# --- AADIDO: Importar las bibliotecas de ploteo al inicio ---\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# --- AADIDO: Importar train_test_split para estratificaci贸n ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. CONFIGURACIN INICIAL ---\n",
    "# ==========================================================\n",
    "# Coloca aqu铆 la ruta a tu modelo .pth (el checkpoint de Lightning)\n",
    "PATH_MODELO_SSL = \"/lustre/proyectos/p032/models/multi_pretext_model2.ckpt\"\n",
    "MODEL_PATH = \"/lustre/home/opacheco/MEDA_Challenge/models/221025MG_backbone.ssl.pth\"\n",
    "\n",
    "# CAMBIADO: Apunta al directorio ra铆z que contiene las 3 carpetas de clases\n",
    "DATASET='/lustre/proyectos/p032/datasets/images/3kvasir' # Opcional, solo para referencia\n",
    "PATH_DATASET = \"/lustre/proyectos/p032/datasets/images/3kvasir\" # <-- 隆CAMBIO IMPORTANTE!\n",
    "\n",
    "# CAMBIADO: Actualizado a 3 clases seg煤n tu descripci贸n\n",
    "NUM_CLASES = 3\n",
    "\n",
    "# Par谩metros\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_DE_PRUEBA = 20\n",
    "LEARNING_RATE = 0.001\n",
    "JIGSAW_N = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# AADIDO: Semilla para reproducibilidad en la divisi贸n\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "# --- 4. Modelo Multi-Pretexto (Versi贸n LightningModule) ---\n",
    "# ... (El c贸digo del modelo es id茅ntico) ...\n",
    "class MultiPretextSSL_Lightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate') # Guarda lr\n",
    "        self.backbone = backbone\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        num_features = 512 # Salida de ResNet18\n",
    "        \n",
    "        # --- DECODER CORREGIDO PARA 28x28 ---\n",
    "        decoder_layers_28x28 = [\n",
    "            nn.ConvTranspose2d(num_features, 256, kernel_size=4, stride=1, padding=0), # 1x1 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1), # 4x4 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),    # 14x14 -> 28x28\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        self.color_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        self.patch_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        \n",
    "        # --- JIGSAW HEAD CORREGIDO PARA N=4 (16 patches) ---\n",
    "        self.n_patches = JIGSAW_N * JIGSAW_N # 16\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_patches * self.n_patches) # 16*16 = 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x, task=\"color\"):\n",
    "        feats = self.backbone(x)\n",
    "        if task == \"color\":\n",
    "            return self.color_head(feats)\n",
    "        elif task == \"patch\":\n",
    "            return self.patch_head(feats)\n",
    "        elif task == \"jigsaw\":\n",
    "            return self.jigsaw_head(feats)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # (El c贸digo es id茅ntico)\n",
    "        imgs = batch\n",
    "        task = random.choice([\"color\", \"patch\", \"jigsaw\"])\n",
    "        # ... (l贸gica de loss) ...\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "class MoCoLightning(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder_q = nn.Sequential(backbone)\n",
    "\n",
    "resnet = models.resnet18(weights=None) \n",
    "# Tu backbone (quitando la capa FC final)\n",
    "backbone_structure = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Cargar el estado\n",
    "encoder_wrapper = MoCoLightning(backbone=backbone_structure)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Cargar los pesos en la estructura\n",
    "encoder_wrapper.encoder_q[0].load_state_dict(state_dict)\n",
    "\n",
    "# --- Este es tu backbone listo para usar ---\n",
    "ssl_backbone = encoder_wrapper.encoder_q[0].to(DEVICE)\n",
    "print(\"Backbone cargado y movido a GPU.\")\n",
    "model = MultiPretextSSL_Lightning.load_from_checkpoint(PATH_MODELO_SSL, backbone=ssl_backbone)\n",
    "\n",
    "# Congelar todo el backbone\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = 512  # ResNet18 sin FC tiene 512 features\n",
    "# Crear la cabeza lineal\n",
    "# --- CAMBIADO: Asegurarse de que NUM_CLASES se actualice si es necesario ---\n",
    "# (La l贸gica de actualizaci贸n ya est谩 abajo, pero la cabeza lineal debe\n",
    "# crearse DESPUS de conocer el NUM_CLASES real)\n",
    "# linear_head = nn.Linear(in_features, NUM_CLASES) # Movido m谩s abajo\n",
    "\n",
    "\n",
    "# --- 5. DEFINIR TRANSFORMACIONES Y CARGAR/DIVIDIR DATASET ---\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(224),               # redimensiona la imagen\n",
    "    transforms.CenterCrop(224),           # recorta el centro\n",
    "    transforms.ToTensor(),                # convierte a tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],    # normalizaci贸n ImageNet\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 6. CREAR DATASETS Y DATALOADERS (LGICA NUEVA) ---\n",
    "\n",
    "try:\n",
    "    full_dataset = ImageFolder(PATH_DATASET, transform=data_transform)\n",
    "    print(f\"Dataset completo cargado desde: {PATH_DATASET}\")\n",
    "    print(f\"Total de im谩genes encontradas: {len(full_dataset)}\")\n",
    "    print(f\"Clases (carpetas) encontradas: {full_dataset.classes}\")\n",
    "    \n",
    "    if len(full_dataset.classes) != NUM_CLASES:\n",
    "        print(f\"隆Advertencia! Se esperaban {NUM_CLASES} clases pero se encontraron {len(full_dataset.classes)} carpetas.\")\n",
    "        NUM_CLASES = len(full_dataset.classes)\n",
    "        print(f\"NUM_CLASES actualizado a {NUM_CLASES}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontr贸 el directorio {PATH_DATASET}\")\n",
    "    print(\"Aseg煤rate de que 'PATH_DATASET' apunte al directorio que contiene las carpetas de tus clases.\")\n",
    "    raise\n",
    "\n",
    "# --- CAMBIADO: L贸gica de random_split reemplazada por train_test_split (estratificado) ---\n",
    "\n",
    "# 1. Definir proporciones (tus proporciones: 30% train, 10% val, 60% test)\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 1.0 - TRAIN_RATIO - VAL_RATIO # 0.6\n",
    "\n",
    "# 2. Obtener etiquetas para estratificaci贸n\n",
    "targets = full_dataset.targets\n",
    "indices = list(range(len(targets)))\n",
    "\n",
    "# 3. Primera divisi贸n (Train vs. Val+Test)\n",
    "# Dividimos para obtener el 30% de train\n",
    "train_indices, val_test_indices, train_targets, val_test_targets = train_test_split(\n",
    "    indices,\n",
    "    targets,\n",
    "    train_size=TRAIN_RATIO,\n",
    "    stratify=targets, # Estratificar en el dataset completo\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# 4. Segunda divisi贸n (Val vs. Test)\n",
    "# Dividimos el 70% restante (val_test_indices) en Val (10%) y Test (60%)\n",
    "# El ratio de Test dentro del subconjunto restante es: 0.6 / (0.1 + 0.6) = 0.6 / 0.7\n",
    "test_split_ratio = TEST_RATIO / (VAL_RATIO + TEST_RATIO) \n",
    "\n",
    "val_indices, test_indices = train_test_split(\n",
    "    val_test_indices,\n",
    "    test_size=test_split_ratio,\n",
    "    stratify=val_test_targets, # Estratificar en el subconjunto\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Dividiendo dataset (estratificado): {len(train_indices)} (Train), {len(val_indices)} (Val), {len(test_indices)} (Test)\")\n",
    "\n",
    "# 5. Crear Datasets usando torch.utils.data.Subset\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# --- FIN DEL CAMBIO DE DIVISIN ---\n",
    "\n",
    "# Crear la cabeza lineal AHORA, asegur谩ndonos de que NUM_CLASES es correcto\n",
    "linear_head = nn.Linear(in_features, NUM_CLASES)\n",
    "\n",
    "class LinearProbingModel(nn.Module):\n",
    "    def __init__(self, backbone, linear_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.linear_head = linear_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)           # [B, 512, 1, 1]\n",
    "        feats = feats.view(feats.size(0), -1)  # Flatten -> [B, 512]\n",
    "        out = self.linear_head(feats)      # [B, NUM_CLASES]\n",
    "        return out\n",
    "    \n",
    "model = LinearProbingModel(model.backbone, linear_head).to(DEVICE)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"DataLoaders creados. Tama帽o train: {len(train_dataset)}, val: {len(val_dataset)}\")\n",
    "\n",
    "# --- ENTRENAR LA CABEZA LINEAL CON VALIDACIN ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.linear_head.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "print(\"Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\")\n",
    "\n",
    "for epoch in range(EPOCHS_DE_PRUEBA):\n",
    "    # ... (El c贸digo de entrenamiento es id茅ntico) ...\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # ---- Train ----\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validaci贸n ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_DE_PRUEBA} - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Entrenamiento de la cabeza finalizado.\")\n",
    "\n",
    "# --- 6. EVALUAR EL RENDIMIENTO ---\n",
    "# CAMBIADO: Mensaje para reflejar la divisi贸n estratificada\n",
    "print(\"Evaluando en el set de testeo (divisi贸n estratificada)...\")\n",
    "\n",
    "model.eval() \n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# --- RESULTADO FINAL ---\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "accuracy = 100 * (all_preds == all_labels).sum() / len(all_labels)\n",
    "print(\"\\n========================================================\")\n",
    "print(f\" 隆Prueba de Evaluaci贸n Lineal (Linear Probing) completa! \")\n",
    "print(f\"    Accuracy en el set de testeo: {accuracy:.2f} %\")\n",
    "print(\"========================================================\")\n",
    "\n",
    "\n",
    "# --- AADIDO: CLCULO Y PLOT DE MATRIZ DE CONFUSIN ---\n",
    "\n",
    "# 1. Obtener los nombres de las clases desde el dataset original\n",
    "#    full_dataset fue creado con ImageFolder y tiene esta informaci贸n\n",
    "class_names = full_dataset.classes\n",
    "print(f\"\\nGenerando matriz de confusi贸n para las clases: {class_names}\")\n",
    "\n",
    "# 2. Calcular la matriz de confusi贸n\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 3. Plotear la matriz usando Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confusi贸n')\n",
    "plt.ylabel('Etiqueta Real (True Label)')\n",
    "plt.xlabel('Etiqueta Predicha (Predicted Label)')\n",
    "plt.show()\n",
    "\n",
    "# --- FIN DE LA MODIFICIN ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fe63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Backbone cargado y movido a GPU.\n",
      "Dataset completo cargado desde: /lustre/proyectos/p032/datasets/images/2kvasir\n",
      "Total de im谩genes encontradas: 1000\n",
      "Clases (carpetas) encontradas: ['esophagitis', 'normal-z-line']\n",
      "Dividiendo dataset: 300 (Train), 10 (Val), 690 (Test)\n",
      "DataLoaders creados. Tama帽o train: 300, val: 10\n",
      "Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\n",
      "Epoch 1/20 - Train Loss: 0.7195 - Val Loss: 0.6879 - Val Acc: 70.00%\n",
      "Epoch 2/20 - Train Loss: 0.7118 - Val Loss: 0.7050 - Val Acc: 50.00%\n",
      "Epoch 3/20 - Train Loss: 0.7063 - Val Loss: 0.6978 - Val Acc: 40.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 246\u001b[0m\n\u001b[1;32m    243\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    245\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 246\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 152\u001b[0m, in \u001b[0;36mLinearProbingModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 152\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# [B, 512, 1, 1]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     feats \u001b[38;5;241m=\u001b[39m feats\u001b[38;5;241m.\u001b[39mview(feats\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten -> [B, 512]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_head(feats)      \u001b[38;5;66;03m# [B, NUM_CLASES]\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/proyectos/p032/env/lib64/python3.9/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "# AADIDO: random_split para dividir el dataset\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "# AADIDO: ImageFolder es la herramienta est谩ndar para tu estructura de dataset\n",
    "from torchvision.datasets import ImageFolder \n",
    "\n",
    "# --- 1. CONFIGURACIN INICIAL ---\n",
    "# ==========================================================\n",
    "# Coloca aqu铆 la ruta a tu modelo .pth (el checkpoint de Lightning)\n",
    "PATH_MODELO_SSL = \"/lustre/proyectos/p032/models/multi_pretext_model2.ckpt\"\n",
    "MODEL_PATH = \"/lustre/home/opacheco/MEDA_Challenge/models/221025MG_backbone.ssl.pth\"\n",
    "\n",
    "# CAMBIADO: Apunta al directorio ra铆z que contiene las 3 carpetas de clases\n",
    "DATASET='kvasir' # Opcional, solo para referencia\n",
    "PATH_DATASET = \"/lustre/proyectos/p032/datasets/images/2kvasir\" # <-- 隆CAMBIO IMPORTANTE!\n",
    "\n",
    "# CAMBIADO: Actualizado a 3 clases seg煤n tu descripci贸n\n",
    "NUM_CLASES = 2\n",
    "\n",
    "# Par谩metros\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_DE_PRUEBA = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "JIGSAW_N = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "# --- 4. Modelo Multi-Pretexto (Versi贸n LightningModule) ---\n",
    "# ... (Todo el c贸digo de MultiPretextSSL_Lightning permanece EXACTAMENTE IGUAL) ...\n",
    "class MultiPretextSSL_Lightning(pl.LightningModule):\n",
    "    def __init__(self, backbone, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate') # Guarda lr\n",
    "        self.backbone = backbone\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        num_features = 512 # Salida de ResNet18\n",
    "        \n",
    "        # --- DECODER CORREGIDO PARA 28x28 ---\n",
    "        decoder_layers_28x28 = [\n",
    "            nn.ConvTranspose2d(num_features, 256, kernel_size=4, stride=1, padding=0), # 1x1 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1), # 4x4 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),    # 14x14 -> 28x28\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        self.color_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        self.patch_head = nn.Sequential(*decoder_layers_28x28)\n",
    "        \n",
    "        # --- JIGSAW HEAD CORREGIDO PARA N=4 (16 patches) ---\n",
    "        self.n_patches = JIGSAW_N * JIGSAW_N # 16\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_patches * self.n_patches) # 16*16 = 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x, task=\"color\"):\n",
    "        feats = self.backbone(x)\n",
    "        if task == \"color\":\n",
    "            return self.color_head(feats)\n",
    "        elif task == \"patch\":\n",
    "            return self.patch_head(feats)\n",
    "        elif task == \"jigsaw\":\n",
    "            return self.jigsaw_head(feats)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch es lo que retorna __getitem__, en este caso, 'imgs'\n",
    "        imgs = batch\n",
    "        \n",
    "        # Elegir una tarea al azar\n",
    "        task = random.choice([\"color\", \"patch\", \"jigsaw\"])\n",
    "        loss = 0.0\n",
    "\n",
    "        if task == \"color\":\n",
    "            inp, target = colorization_pair_tensor(imgs)\n",
    "            pred = self(inp, \"color\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"patch\":\n",
    "            inp, target = patch_prediction_pair_tensor(imgs)\n",
    "            pred = self(inp, \"patch\")\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        elif task == \"jigsaw\":\n",
    "            inp, target = jigsaw_pair_tensor(imgs, n=JIGSAW_N) # n=4\n",
    "            pred = self(inp, \"jigsaw\") # Shape: [B, 256]\n",
    "            \n",
    "            # [B, 256] -> [B, 16, 16]\n",
    "            pred_reshaped = pred.view(-1, self.n_patches, self.n_patches)  \n",
    "            target_reshaped = target.view(-1) # [B*16]\n",
    "            \n",
    "            loss = F.cross_entropy(pred_reshaped.view(-1, self.n_patches), target_reshaped)\n",
    "\n",
    "        # Loggear la p茅rdida. 'prog_bar=True' la muestra en la barra de progreso\n",
    "        self.log(f'loss_{task}', loss, prog_bar=True)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "class MoCoLightning(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder_q = nn.Sequential(backbone)\n",
    "\n",
    "resnet = models.resnet18(weights=None) \n",
    "# Tu backbone (quitando la capa FC final)\n",
    "backbone_structure = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Cargar el estado\n",
    "encoder_wrapper = MoCoLightning(backbone=backbone_structure)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# Cargar los pesos en la estructura\n",
    "encoder_wrapper.encoder_q[0].load_state_dict(state_dict)\n",
    "\n",
    "# --- Este es tu backbone listo para usar ---\n",
    "ssl_backbone = encoder_wrapper.encoder_q[0].to(DEVICE)\n",
    "print(\"Backbone cargado y movido a GPU.\")\n",
    "model = MultiPretextSSL_Lightning.load_from_checkpoint(PATH_MODELO_SSL, backbone=ssl_backbone)\n",
    "\n",
    "# Congelar todo el backbone\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = 512  # ResNet18 sin FC tiene 512 features\n",
    "# Crear la cabeza lineal\n",
    "linear_head = nn.Linear(in_features, NUM_CLASES)\n",
    "\n",
    "class LinearProbingModel(nn.Module):\n",
    "    def __init__(self, backbone, linear_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.linear_head = linear_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)           # [B, 512, 1, 1]\n",
    "        feats = feats.view(feats.size(0), -1)  # Flatten -> [B, 512]\n",
    "        out = self.linear_head(feats)      # [B, NUM_CLASES]\n",
    "        return out\n",
    "\n",
    "model = LinearProbingModel(model.backbone, linear_head).to(DEVICE)\n",
    "\n",
    "\n",
    "# --- BLOQUE NPZ ELIMINADO ---\n",
    "# ya no cargamos 'data = np.load(PATH_DATASET)'\n",
    "# ya no extraemos 'x_train', 'y_train', etc.\n",
    "\n",
    "\n",
    "# --- 5. DEFINIR TRANSFORMACIONES Y CARGAR/DIVIDIR DATASET ---\n",
    "\n",
    "# Las transformaciones son las mismas, ImageFolder las aplicar谩\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(224),               # redimensiona la imagen\n",
    "    transforms.CenterCrop(224),           # recorta el centro\n",
    "    transforms.ToTensor(),                # convierte a tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],    # normalizaci贸n ImageNet\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# --- CLASE DatasetWrapper ELIMINADA ---\n",
    "# Ya no es necesaria, ImageFolder hace este trabajo.\n",
    "\n",
    "\n",
    "# --- 6. CREAR DATASETS Y DATALOADERS (LGICA NUEVA) ---\n",
    "\n",
    "# AADIDO: Cargar el dataset completo desde el directorio\n",
    "try:\n",
    "    full_dataset = ImageFolder(PATH_DATASET, transform=data_transform)\n",
    "    print(f\"Dataset completo cargado desde: {PATH_DATASET}\")\n",
    "    print(f\"Total de im谩genes encontradas: {len(full_dataset)}\")\n",
    "    print(f\"Clases (carpetas) encontradas: {full_dataset.classes}\")\n",
    "    \n",
    "    # Verificar que el n煤mero de clases coincida\n",
    "    if len(full_dataset.classes) != NUM_CLASES:\n",
    "        print(f\"隆Advertencia! Se esperaban {NUM_CLASES} clases pero se encontraron {len(full_dataset.classes)} carpetas.\")\n",
    "        NUM_CLASES = len(full_dataset.classes) # Opcional: ajustar autom谩ticamente\n",
    "        print(f\"NUM_CLASES actualizado a {NUM_CLASES}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontr贸 el directorio {PATH_DATASET}\")\n",
    "    print(\"Aseg煤rate de que 'PATH_DATASET' apunte al directorio que contiene las carpetas de tus clases.\")\n",
    "    # Detener la ejecuci贸n si el path es incorrecto\n",
    "    raise\n",
    "\n",
    "# AADIDO: Definir proporciones para la divisi贸n (ej. 70% train, 15% val, 15% test)\n",
    "train_ratio = 0.30\n",
    "val_ratio = 0.01\n",
    "# test_ratio se infiere (1.0 - train_ratio - val_ratio)\n",
    "\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "val_size = int(val_ratio * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "print(f\"Dividiendo dataset: {train_size} (Train), {val_size} (Val), {test_size} (Test)\")\n",
    "\n",
    "# AADIDO: Dividir el dataset aleatoriamente\n",
    "# Es importante fijar una semilla (generator) para reproducibilidad si se desea\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42) # Opcional: para resultados reproducibles\n",
    ")\n",
    "\n",
    "# La creaci贸n de DataLoaders es la misma, pero ahora usa los subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print(f\"DataLoaders creados. Tama帽o train: {len(train_dataset)}, val: {len(val_dataset)}\")\n",
    "\n",
    "# --- ENTRENAR LA CABEZA LINEAL CON VALIDACIN ---\n",
    "# ... (Todo el c贸digo de entrenamiento permanece EXACTAMENTE IGUAL) ...\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.linear_head.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "print(\"Iniciando entrenamiento de la cabeza lineal (Linear Probing)...\")\n",
    "\n",
    "for epoch in range(EPOCHS_DE_PRUEBA):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # ---- Train ----\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validaci贸n ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_DE_PRUEBA} - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Entrenamiento de la cabeza finalizado.\")\n",
    "\n",
    "# --- EVALUAR EL RENDIMIENTO ---\n",
    "# ... (Todo el c贸digo de evaluaci贸n permanece EXACTAMENTE IGUAL) ...\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 6. EVALUAR EL RENDIMIENTO ---\n",
    "print(\"Evaluando en el set de testeo (divisi贸n aleatoria)...\")\n",
    "\n",
    "model.eval() \n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# --- RESULTADO FINAL ---\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "accuracy = 100 * (all_preds == all_labels).sum() / len(all_labels)\n",
    "print(\"\\n========================================================\")\n",
    "print(f\" 隆Prueba de Evaluaci贸n Lineal (Linear Probing) completa! \")\n",
    "print(f\"    Accuracy en el set de testeo: {accuracy:.2f} %\")\n",
    "print(\"========================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
